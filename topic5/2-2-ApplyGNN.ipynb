{"cells":[{"cell_type":"markdown","id":"8f4a91d8","metadata":{"id":"8f4a91d8"},"source":["# Applying GNN Models\n","\n","In this lecture, we will continue using the Cora example from the previous lesson. You will learn about:\n","\n","- Unsupervised GRL\n","- GNNs for Supervised Downstream Tasks\n","\n","We will also compare these methods with the approach discussed in the previous lesson."]},{"cell_type":"code","execution_count":195,"id":"fb43d373","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25952,"status":"ok","timestamp":1663384864744,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"fb43d373","outputId":"836e9449-86ab-48cd-d998-5fd0622d55a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.3.0\n"]}],"source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","# !pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n","\n","import os.path as osp\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.transforms import NormalizeFeatures\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.linear_model import LogisticRegression\n","from torch_geometric.loader import LinkNeighborLoader"]},{"cell_type":"markdown","id":"a9e4160e","metadata":{},"source":["## Unsupervised Graph Representation Learning with GraphSAGE\n","Since we aim to learn graph representations through an unsupervised method, we do not use node labels for training.<br>\n","\n","We assume that if there is a link between a pair of nodes, those nodes should have similar embeddings. Conversely, if there is no link between a pair of nodes, their embeddings should be dissimilar.\n","\n","Based on this assumption, we can define the following loss function:\n"," \n","\\begin{equation}\n","\\text{Loss} = - \\left( \\log \\left( \\sigma(h_u^{\\top} h_v) \\right) - \\sum_{i=1}^k \\log \\left( \\sigma(h_u^{\\top} h_{n_i}) \\right) \\right) , n_i \\sim P_V\n","\\end{equation}\n","\n","- $\\log \\left( \\sigma (h_u^{\\top} h_v) \\right)$:  The similarity between the positive sample pair (i.e., true neighbors). Maximizing this term means you want the similarity of positive samples to be as high as possible.\n","  \n","- $- \\sum_{i=1}^k \\log \\left( \\sigma (h_u^{\\top} h_{n_i}) \\right)$: The similarity between the negative sample pairs (i.e., non-neighbors). Minimizing this term means you want the similarity of negative samples to be as low as possible.\n","\n","Once the embeddings are obtained, they are fed into an additional classifier for node classification."]},{"cell_type":"code","execution_count":196,"id":"YbBjup3U9MQk","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3020,"status":"ok","timestamp":1663386340276,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"YbBjup3U9MQk","outputId":"465f6186-36da-486b-c75f-aceb8876cb54"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","dataset = 'Cora'\n","path = osp.join('.', 'data', dataset)\n","dataset = Planetoid(root=path, name='Cora', transform=NormalizeFeatures())\n","data = dataset[0]\n","print(data)"]},{"cell_type":"code","execution_count":197,"id":"e0c194b0","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663386341904,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"e0c194b0"},"outputs":[],"source":["from torch_geometric.nn import SAGEConv\n","\n","class GraphSAGE(nn.Module):\n","    def __init__(self, in_channels, hidden_channels, num_layers):\n","        super().__init__()\n","        self.num_layers = num_layers\n","        self.convs = nn.ModuleList()\n","        for i in range(num_layers):\n","            in_channels = in_channels if i == 0 else hidden_channels\n","            self.convs.append(SAGEConv(in_channels, hidden_channels))\n","\n","    def forward(self, x, edge_index):\n","        for i, conv in enumerate(self.convs):\n","            x = conv(x, edge_index)\n","            if i != self.num_layers - 1:\n","                x = F.relu(x)\n","                x = F.dropout(x, p=0.5, training=self.training)\n","        return x"]},{"cell_type":"code","execution_count":198,"id":"583902a9","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663386459760,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"583902a9"},"outputs":[],"source":["# define neighbor sampler\n","train_loader = LinkNeighborLoader(\n","    data,\n","    batch_size=256,\n","    shuffle=True,\n","    neg_sampling_ratio=1.0,\n","    num_neighbors=[10, 10],\n",")"]},{"cell_type":"code","execution_count":199,"id":"BK7x8-jo9MQl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1663386460858,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"BK7x8-jo9MQl","outputId":"823cdf0f-de02-47c1-8696-70e42e26b13c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data(x=[2405, 1433], edge_index=[2, 7862], y=[2405], train_mask=[2405], val_mask=[2405], test_mask=[2405], n_id=[2405], e_id=[7862], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[256], edge_label_index=[2, 512], edge_label=[512])\n"]}],"source":["for batch in train_loader:\n","    print(batch)\n","    break"]},{"cell_type":"code","execution_count":200,"id":"kWzD1vPb9MQl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283,"status":"ok","timestamp":1663386488587,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"kWzD1vPb9MQl","outputId":"ae18e461-89fa-43c3-c0af-16f1d6d005a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Edge label index: containing both positive and negative edges\n","torch.Size([2, 512])\n","Edge label: 1 stands for positive and 0 stands for negative node pair(edge)\n"]}],"source":["print(\"Edge label index: containing both positive and negative edges\")\n","print(batch.edge_label_index)\n","\n","print(\"Edge label: 1 stands for positive and 0 stands for negative node pair(edge)\")\n","print(batch.edge_label)"]},{"cell_type":"code","execution_count":201,"id":"b5890c80","metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = GraphSAGE(data.num_node_features, hidden_channels=64, num_layers=2)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n","model = model.to(device)\n","x, edge_index = data.x.to(device), data.edge_index.to(device)"]},{"cell_type":"code","execution_count":202,"id":"BaT6bJq89MQl","metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1663386518319,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"BaT6bJq89MQl"},"outputs":[],"source":["# define training and testing functions\n","def train():\n","    model.train()\n","\n","    total_loss = 0\n","    for batch in train_loader:\n","        batch = batch.to(device)\n","        optimizer.zero_grad()\n","        embedding = model(batch.x, batch.edge_index)\n","        embedding_src = embedding[batch.edge_label_index[0]]\n","        embedding_dst = embedding[batch.edge_label_index[1]]\n","        pred = (embedding_src * embedding_dst).sum(dim=-1)\n","        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += float(loss) * pred.size(0)\n","\n","    return total_loss / data.num_nodes\n","\n","\n","@torch.no_grad()\n","def test():\n","    model.eval()\n","    out = model(data.x.to(device), data.edge_index.to(device)).cpu() \n","\n","    clf = LogisticRegression()\n","    clf.fit(out[data.train_mask], data.y[data.train_mask])\n","\n","    val_acc = clf.score(out[data.val_mask], data.y[data.val_mask])\n","    test_acc = clf.score(out[data.test_mask], data.y[data.test_mask])\n","\n","    return val_acc, test_acc"]},{"cell_type":"code","execution_count":203,"id":"e88903af","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78288,"status":"ok","timestamp":1663386603993,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"e88903af","outputId":"a9284f85-5802-437a-eb7c-c4cba8e390ac","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([120, 64])\n","torch.Size([120])\n","Epoch: 001, Loss: 5.2622, Val: 0.5220, Test: 0.4890\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([120, 64])\n","torch.Size([120])\n","Epoch: 002, Loss: 4.3896, Val: 0.6160, Test: 0.6430\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([120, 64])\n","torch.Size([120])\n","Epoch: 003, Loss: 4.0817, Val: 0.6440, Test: 0.6720\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([120, 64])\n","torch.Size([120])\n","Epoch: 004, Loss: 3.9840, Val: 0.6740, Test: 0.7080\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([120, 64])\n","torch.Size([120])\n","Epoch: 005, Loss: 3.9165, Val: 0.6900, Test: 0.7220\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([120, 64])\n","torch.Size([120])\n","Epoch: 006, Loss: 3.9026, Val: 0.6840, Test: 0.7190\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([120, 64])\n","torch.Size([120])\n","Epoch: 007, Loss: 3.8673, Val: 0.7020, Test: 0.7330\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n","torch.Size([512])\n","torch.Size([512, 64])\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[203], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     val_acc, test_acc \u001b[38;5;241m=\u001b[39m test()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[202], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m pred \u001b[38;5;241m=\u001b[39m (embedding_src \u001b[38;5;241m*\u001b[39m embedding_dst)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(pred, batch\u001b[38;5;241m.\u001b[39medge_label)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred\u001b[38;5;241m.\u001b[39mshape)\n","File \u001b[0;32m~/anaconda3/envs/py-3.9/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/py-3.9/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/py-3.9/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for epoch in range(1, 101):\n","    loss = train()\n","    val_acc, test_acc = test()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n","          f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"]},{"cell_type":"markdown","id":"6caee53d","metadata":{"id":"6caee53d"},"source":["## Performance comparison\n","Recall that in the previous examples, we performed node classification in 3 different ways.\n","1. Bag of word + MLP with `Accuracy:0.6`\n","2. Node2vec + Logistic regression with `Accuracy:0.703`\n","3. Node2vec with Bag of word + Logistic regression with `Accuracy:0.707`\n","4. GraphSAGE: with Bag of word + Logistic regression with `Accuracy:0.803`\n","\n","We make use of node feature and graph structure at the same time and boost the accuracy up to **0.791** with a simple two-layer `GraphSAGE`."]},{"cell_type":"markdown","id":"36240400","metadata":{"id":"36240400"},"source":["## End-to-end semi-supervised learning with Graph Convolution Network(GCN)\n","Previously, we adopt a two stage classification pipeline where we first extract network feature via unsupervised learning then utilize a classifier to predict its label. <br>\n","The two-stage design could be suboptimal since the network features were not extracted for specific task. <br>\n","Therefore, we now employ an end-to-end approach. This ensures that the features learned by the model are directly aligned with the specific task, potentially leading to better performance."]},{"cell_type":"code","execution_count":146,"id":"9b156ae0","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1663386768615,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"9b156ae0"},"outputs":[],"source":["from torch_geometric.nn import GCNConv\n","import torch.nn.functional as F\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels):\n","        super().__init__()\n","        self.conv1 = GCNConv(in_channels, hidden_channels, cached=True,\n","                             normalize=True)\n","        self.conv2 = GCNConv(hidden_channels, out_channels, cached=True,\n","                             normalize=True)\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        x = F.dropout(x, p=0.3, training=self.training)\n","        x = self.conv1(x, edge_index, edge_weight).relu()\n","        x = F.dropout(x, p=0.3, training=self.training)\n","        x = self.conv2(x, edge_index, edge_weight)\n","        return x"]},{"cell_type":"code","execution_count":147,"id":"67338214","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663386768902,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"67338214","outputId":"07232e0d-90e6-43ef-e805-859283b424b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["GCN(\n","  (conv1): GCNConv(1433, 64)\n","  (conv2): GCNConv(64, 7)\n",")\n"]}],"source":["dim = 64\n","model = GCN(dataset.num_features, dim, dataset.num_classes)\n","model, data = model.to(device), data.to(device)\n","optimizer = torch.optim.Adam(model.parameters(),weight_decay=1e-4)\n","print(model)"]},{"cell_type":"code","execution_count":148,"id":"646fb96c","metadata":{"executionInfo":{"elapsed":268,"status":"ok","timestamp":1663386794576,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"646fb96c"},"outputs":[],"source":["def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data.x, data.edge_index, data.edge_weight)\n","    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    return float(loss)\n","\n","@torch.no_grad()\n","def test():\n","    model.eval()\n","    pred = model(data.x, data.edge_index, data.edge_weight).argmax(dim=-1)\n","    mask = data.test_mask\n","    accs = (int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n","    return accs"]},{"cell_type":"code","execution_count":149,"id":"142e2595","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2101,"status":"ok","timestamp":1663386797988,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"142e2595","outputId":"9596e72b-144a-43f7-ba6f-078dcc2338e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss:1.9459 Testing accuracy:0.2200\n","Loss:1.9444 Testing accuracy:0.2780\n","Loss:1.9421 Testing accuracy:0.3260\n","Loss:1.9406 Testing accuracy:0.3770\n","Loss:1.9385 Testing accuracy:0.4440\n","Loss:1.9361 Testing accuracy:0.5290\n","Loss:1.9345 Testing accuracy:0.5850\n","Loss:1.9323 Testing accuracy:0.6350\n","Loss:1.9303 Testing accuracy:0.6650\n","Loss:1.9275 Testing accuracy:0.6890\n","Loss:1.9256 Testing accuracy:0.7080\n","Loss:1.9232 Testing accuracy:0.7240\n","Loss:1.9202 Testing accuracy:0.7280\n","Loss:1.9180 Testing accuracy:0.7330\n","Loss:1.9158 Testing accuracy:0.7380\n","Loss:1.9134 Testing accuracy:0.7470\n","Loss:1.9078 Testing accuracy:0.7490\n","Loss:1.9077 Testing accuracy:0.7590\n","Loss:1.9036 Testing accuracy:0.7710\n","Loss:1.9010 Testing accuracy:0.7800\n","Loss:1.8970 Testing accuracy:0.7870\n","Loss:1.8955 Testing accuracy:0.7860\n","Loss:1.8907 Testing accuracy:0.7990\n","Loss:1.8885 Testing accuracy:0.7970\n","Loss:1.8839 Testing accuracy:0.7950\n","Loss:1.8834 Testing accuracy:0.7970\n","Loss:1.8788 Testing accuracy:0.8000\n","Loss:1.8776 Testing accuracy:0.8000\n","Loss:1.8707 Testing accuracy:0.7940\n","Loss:1.8669 Testing accuracy:0.7960\n","Loss:1.8644 Testing accuracy:0.7910\n","Loss:1.8596 Testing accuracy:0.7890\n","Loss:1.8585 Testing accuracy:0.7880\n","Loss:1.8518 Testing accuracy:0.7880\n","Loss:1.8499 Testing accuracy:0.7870\n","Loss:1.8465 Testing accuracy:0.7870\n","Loss:1.8431 Testing accuracy:0.7870\n","Loss:1.8387 Testing accuracy:0.7880\n","Loss:1.8336 Testing accuracy:0.7880\n","Loss:1.8290 Testing accuracy:0.7890\n","Loss:1.8271 Testing accuracy:0.7890\n","Loss:1.8220 Testing accuracy:0.7950\n","Loss:1.8174 Testing accuracy:0.7940\n","Loss:1.8114 Testing accuracy:0.7970\n","Loss:1.8105 Testing accuracy:0.8020\n","Loss:1.8027 Testing accuracy:0.8020\n","Loss:1.7972 Testing accuracy:0.8030\n","Loss:1.7982 Testing accuracy:0.8040\n","Loss:1.7923 Testing accuracy:0.8050\n","Loss:1.7804 Testing accuracy:0.8050\n","Loss:1.7804 Testing accuracy:0.8050\n","Loss:1.7721 Testing accuracy:0.8060\n","Loss:1.7699 Testing accuracy:0.8060\n","Loss:1.7636 Testing accuracy:0.8060\n","Loss:1.7611 Testing accuracy:0.8050\n","Loss:1.7539 Testing accuracy:0.8060\n","Loss:1.7529 Testing accuracy:0.8050\n","Loss:1.7405 Testing accuracy:0.8050\n","Loss:1.7345 Testing accuracy:0.8060\n","Loss:1.7301 Testing accuracy:0.8070\n","Loss:1.7253 Testing accuracy:0.8090\n","Loss:1.7275 Testing accuracy:0.8130\n","Loss:1.7277 Testing accuracy:0.8120\n","Loss:1.7118 Testing accuracy:0.8120\n","Loss:1.7101 Testing accuracy:0.8120\n","Loss:1.7019 Testing accuracy:0.8110\n","Loss:1.6972 Testing accuracy:0.8110\n","Loss:1.6954 Testing accuracy:0.8120\n","Loss:1.6861 Testing accuracy:0.8110\n","Loss:1.6787 Testing accuracy:0.8110\n","Loss:1.6830 Testing accuracy:0.8100\n","Loss:1.6692 Testing accuracy:0.8080\n","Loss:1.6663 Testing accuracy:0.8080\n","Loss:1.6626 Testing accuracy:0.8070\n","Loss:1.6570 Testing accuracy:0.8060\n","Loss:1.6433 Testing accuracy:0.8060\n","Loss:1.6361 Testing accuracy:0.8040\n","Loss:1.6374 Testing accuracy:0.8010\n","Loss:1.6264 Testing accuracy:0.8000\n","Loss:1.6184 Testing accuracy:0.7990\n","Loss:1.6190 Testing accuracy:0.8010\n","Loss:1.6088 Testing accuracy:0.8020\n","Loss:1.5980 Testing accuracy:0.8030\n","Loss:1.5978 Testing accuracy:0.8040\n","Loss:1.5867 Testing accuracy:0.8060\n","Loss:1.5843 Testing accuracy:0.8080\n","Loss:1.5756 Testing accuracy:0.8080\n","Loss:1.5686 Testing accuracy:0.8080\n","Loss:1.5653 Testing accuracy:0.8070\n","Loss:1.5519 Testing accuracy:0.8070\n","Loss:1.5564 Testing accuracy:0.8070\n","Loss:1.5349 Testing accuracy:0.8070\n","Loss:1.5379 Testing accuracy:0.8080\n","Loss:1.5291 Testing accuracy:0.8100\n","Loss:1.5183 Testing accuracy:0.8090\n","Loss:1.5161 Testing accuracy:0.8090\n","Loss:1.5232 Testing accuracy:0.8120\n","Loss:1.4982 Testing accuracy:0.8110\n","Loss:1.4979 Testing accuracy:0.8110\n","Loss:1.4882 Testing accuracy:0.8100\n","Loss:1.4780 Testing accuracy:0.8090\n","Loss:1.4841 Testing accuracy:0.8090\n","Loss:1.4713 Testing accuracy:0.8090\n","Loss:1.4614 Testing accuracy:0.8080\n","Loss:1.4652 Testing accuracy:0.8070\n","Loss:1.4496 Testing accuracy:0.8060\n","Loss:1.4435 Testing accuracy:0.8070\n","Loss:1.4365 Testing accuracy:0.8060\n","Loss:1.4157 Testing accuracy:0.8060\n","Loss:1.4337 Testing accuracy:0.8040\n","Loss:1.4126 Testing accuracy:0.8050\n","Loss:1.4168 Testing accuracy:0.8040\n","Loss:1.4085 Testing accuracy:0.8030\n","Loss:1.3975 Testing accuracy:0.8030\n","Loss:1.3820 Testing accuracy:0.8040\n","Loss:1.3876 Testing accuracy:0.8040\n","Loss:1.3733 Testing accuracy:0.8020\n","Loss:1.3604 Testing accuracy:0.8030\n","Loss:1.3664 Testing accuracy:0.8020\n","Loss:1.3531 Testing accuracy:0.8020\n","Loss:1.3323 Testing accuracy:0.8020\n","Loss:1.3421 Testing accuracy:0.8020\n","Loss:1.3199 Testing accuracy:0.8020\n","Loss:1.3196 Testing accuracy:0.8030\n","Loss:1.3095 Testing accuracy:0.8030\n","Loss:1.3140 Testing accuracy:0.8050\n","Loss:1.2832 Testing accuracy:0.8050\n","Loss:1.3065 Testing accuracy:0.8050\n","Loss:1.2820 Testing accuracy:0.8060\n","Loss:1.2792 Testing accuracy:0.8080\n","Loss:1.2604 Testing accuracy:0.8090\n","Loss:1.2793 Testing accuracy:0.8080\n","Loss:1.2591 Testing accuracy:0.8080\n","Loss:1.2468 Testing accuracy:0.8090\n","Loss:1.2366 Testing accuracy:0.8090\n","Loss:1.2195 Testing accuracy:0.8090\n","Loss:1.2408 Testing accuracy:0.8100\n","Loss:1.2069 Testing accuracy:0.8100\n","Loss:1.2212 Testing accuracy:0.8080\n","Loss:1.2099 Testing accuracy:0.8070\n","Loss:1.1934 Testing accuracy:0.8070\n","Loss:1.2043 Testing accuracy:0.8090\n","Loss:1.1777 Testing accuracy:0.8110\n","Loss:1.1915 Testing accuracy:0.8100\n","Loss:1.1668 Testing accuracy:0.8100\n","Loss:1.1775 Testing accuracy:0.8100\n","Loss:1.1585 Testing accuracy:0.8100\n","Loss:1.1485 Testing accuracy:0.8110\n","Loss:1.1588 Testing accuracy:0.8100\n","Loss:1.1105 Testing accuracy:0.8110\n","Loss:1.1482 Testing accuracy:0.8120\n","Loss:1.1095 Testing accuracy:0.8120\n","Loss:1.0980 Testing accuracy:0.8120\n","Loss:1.1215 Testing accuracy:0.8120\n","Loss:1.0972 Testing accuracy:0.8120\n","Loss:1.1088 Testing accuracy:0.8110\n","Loss:1.1101 Testing accuracy:0.8110\n","Loss:1.0857 Testing accuracy:0.8110\n","Loss:1.0794 Testing accuracy:0.8110\n","Loss:1.0702 Testing accuracy:0.8110\n","Loss:1.0404 Testing accuracy:0.8100\n","Loss:1.0594 Testing accuracy:0.8090\n","Loss:1.0553 Testing accuracy:0.8100\n","Loss:1.0253 Testing accuracy:0.8090\n","Loss:1.0395 Testing accuracy:0.8080\n","Loss:1.0416 Testing accuracy:0.8080\n","Loss:1.0273 Testing accuracy:0.8080\n","Loss:1.0003 Testing accuracy:0.8090\n","Loss:1.0078 Testing accuracy:0.8100\n","Loss:1.0062 Testing accuracy:0.8090\n","Loss:0.9919 Testing accuracy:0.8110\n","Loss:0.9904 Testing accuracy:0.8100\n","Loss:0.9988 Testing accuracy:0.8110\n","Loss:0.9735 Testing accuracy:0.8100\n","Loss:0.9770 Testing accuracy:0.8100\n","Loss:0.9530 Testing accuracy:0.8100\n","Loss:0.9808 Testing accuracy:0.8110\n","Loss:0.9370 Testing accuracy:0.8120\n","Loss:0.9691 Testing accuracy:0.8120\n","Loss:0.9390 Testing accuracy:0.8140\n","Loss:0.9498 Testing accuracy:0.8140\n","Loss:0.9392 Testing accuracy:0.8130\n","Loss:0.9202 Testing accuracy:0.8130\n","Loss:0.9083 Testing accuracy:0.8120\n","Loss:0.9105 Testing accuracy:0.8120\n","Loss:0.8948 Testing accuracy:0.8130\n","Loss:0.9119 Testing accuracy:0.8130\n","Loss:0.9040 Testing accuracy:0.8130\n","Loss:0.8861 Testing accuracy:0.8130\n","Loss:0.8952 Testing accuracy:0.8130\n","Loss:0.8695 Testing accuracy:0.8140\n","Loss:0.8593 Testing accuracy:0.8140\n","Loss:0.8610 Testing accuracy:0.8120\n","Loss:0.8541 Testing accuracy:0.8120\n","Loss:0.8326 Testing accuracy:0.8110\n","Loss:0.8683 Testing accuracy:0.8110\n","Loss:0.8436 Testing accuracy:0.8110\n","Loss:0.8555 Testing accuracy:0.8100\n","Loss:0.8353 Testing accuracy:0.8110\n","Loss:0.8042 Testing accuracy:0.8110\n"]}],"source":["for epoch in range(200):\n","    loss = train()\n","    test_acc = test()\n","    print(f\"Loss:{loss:.4f} Testing accuracy:{test_acc:.4f}\")"]},{"cell_type":"markdown","id":"ec8ff377","metadata":{"id":"ec8ff377"},"source":["## Performance comparison\n","Recall that in the previous examples, we performed node classification in 3 different ways.\n","1. Bag of word + MLP with `Accuracy:0.6`\n","2. Node2vec + Logistic regression with `Accuracy:0.703`\n","3. Node2vec with Bag of word + Logistic regression with `Accuracy:0.707`\n","4. GraphSAGE: with Bag of word + Logistic regression with `Accuracy:0.803`\n","4. GCN: end-to-end learning with `Accuracy:0.818`\n","\n","From this example, we clearly figure out that using GCN with end2end training leads to the best performance since the feature extracted could be learned to optimize the node classification task."]},{"cell_type":"markdown","id":"d347b13c","metadata":{"id":"d347b13c"},"source":["## Applying different GNN backbone layer\n","The full list of implemented GNN could be found in [here.](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers)"]},{"cell_type":"code","execution_count":150,"id":"765f7d34","metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1663387016492,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"765f7d34"},"outputs":[],"source":["from torch_geometric.nn import GCNConv,GATConv, GraphSAGE\n","import torch.nn.functional as F\n","\n","class GNN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, gnn_type):\n","        super().__init__()\n","        if gnn_type == \"GCN\":\n","            self.GNN = GCNConv\n","        elif gnn_type == \"SAGE\":\n","            self.GNN = GraphSAGE\n","        elif gnn_type == \"GAT\":\n","            self.GNN = GATConv\n","        \n","        self.conv1 = self.GNN(in_channels, hidden_channels)\n","        self.conv2 = self.GNN(hidden_channels, out_channels)\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv1(x, edge_index, edge_weight).relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index, edge_weight)\n","        return x"]},{"cell_type":"code","execution_count":151,"id":"f37bd755","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1663387016912,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"f37bd755","outputId":"7c33b9d5-4dd0-4305-ca6e-c1bee2a074d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["GNN(\n","  (conv1): GATConv(1433, 32, heads=1)\n","  (conv2): GATConv(32, 7, heads=1)\n",")\n"]}],"source":["dim = 32\n","gnn_type = \"GAT\"\n","model = GNN(dataset.num_features, dim, dataset.num_classes,gnn_type=gnn_type)\n","model, data = model.to(device), data.to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","print(model)"]},{"cell_type":"markdown","id":"afe00d31","metadata":{"id":"afe00d31"},"source":["# HW: Link prediction with GNN\n","1. Try different GNN layer\n","2. Try to optimize the performance by stacking multiple layers\n","3. Report the best accuracy on testing set and the best model configuration(e.g., how many layers?)"]},{"cell_type":"code","execution_count":152,"id":"cfb91924","metadata":{"executionInfo":{"elapsed":281,"status":"ok","timestamp":1663387050805,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"cfb91924"},"outputs":[],"source":["# Let's practice how to use GNN for link prediction\n","# First we need to load the Cora dataset\n","\n","from sklearn.metrics import roc_auc_score\n","import torch_geometric.transforms as T\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.utils import negative_sampling\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","transform = T.Compose([\n","    T.NormalizeFeatures(),\n","    T.ToDevice(device),\n","    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n","                      add_negative_train_samples=True),\n","])\n","dataset = Planetoid(path, name='Cora', transform=transform)\n","train_data, val_data, test_data = dataset[0]"]},{"cell_type":"code","execution_count":153,"id":"43bed8c6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1663387060011,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"43bed8c6","outputId":"24659c22-d453-4534-db45-9d742e94c9b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------Training data------\n","Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[8976], edge_label_index=[2, 8976])\n","Training edges:\n","tensor([[ 154,  727,  901,  ..., 1363,  370, 2037],\n","        [1752, 2570, 2186,  ...,  524, 2206, 1884]], device='cuda:0')\n","Labels\n","tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0')\n","\n","--------Testing data------\n","Data(x=[2708, 1433], edge_index=[2, 9502], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054])\n","Testing edges:\n","tensor([[ 963,  942, 1257,  ..., 2237, 1533, 1957],\n","        [1703, 1924, 2678,  ..., 1710, 2552, 2619]], device='cuda:0')\n","Labels\n","tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0')\n"]}],"source":["print(\"--------Training data------\")\n","print(train_data)\n","print(\"Training edges:\")\n","print(train_data.edge_label_index)\n","print(\"Labels\")\n","print(train_data.edge_label)\n","\n","print()\n","print(\"--------Testing data------\")\n","print(test_data)\n","print(\"Testing edges:\")\n","print(test_data.edge_label_index)\n","print(\"Labels\")\n","print(test_data.edge_label)"]},{"cell_type":"code","execution_count":154,"id":"8ccc3487","metadata":{"executionInfo":{"elapsed":1021,"status":"ok","timestamp":1663389253599,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"8ccc3487"},"outputs":[],"source":["class MyGNN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels):\n","        super().__init__()\n","        ############################################################################\n","        # TODO: Your code here! \n","        # create you GNN layer here. \n","        # try to use different GNN backbone layer or stacking multiple layer to boost performance\n","        self.conv1 = GCNConv(in_channels, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, out_channels)\n","        \n","        ############################################################################\n","\n","    def forward(self, x, edge_index):\n","        ############################################################################\n","        # TODO: Your code here! \n","        # Apply the forward pass according to your GNN layers\n","        # you shoud return the embedding of each node (x has shape [num_nodes, dim])    \n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = self.conv2(x, edge_index)  \n","        ############################################################################\n","        return x\n","    \n","    def get_prediction(self, node_embedding, edges):\n","        # In this function, we have the node embedding and edges as input\n","        # Input shapes:\n","        #      node_embedding: (|V|, out_channels)\n","        #      edges: (2, number of edges)\n","        # To generate such output, we use the inner product of embeddings of two nodes\n","        # The output is to generate a scalar for each pair of edge\n","        embedding_first_node = node_embedding[edges[0]]\n","        embedding_second_node = node_embedding[edges[1]]\n","        ############################################################################\n","        # TODO: Your code here! \n","        # implement the element-wise product as edge feature for link prediction\n","        inner_product = torch.sum(embedding_first_node * embedding_second_node, dim=-1)\n","        \n","        ############################################################################\n","        return inner_product"]},{"cell_type":"code","execution_count":155,"id":"8236b1d0","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663389254662,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"8236b1d0"},"outputs":[],"source":["############################################################################\n","# TODO: Your code here! \n","# initiate your GNN model and select the criterion for link prediction\n","\n","model = MyGNN(dataset.num_features, 128, 64).to(device)\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","############################################################################"]},{"cell_type":"code","execution_count":156,"id":"52305879","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663389254663,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"52305879"},"outputs":[],"source":["# Implement the train function\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    embedding = model(train_data.x, train_data.edge_index)\n","\n","    # We perform a new round of negative sampling for every training epoch:\n","    neg_edge_index = negative_sampling(\n","        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n","        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n","\n","    edge_label_index = torch.cat(\n","        [train_data.edge_label_index, neg_edge_index],\n","        dim=-1,\n","    )\n","    \n","    # Please assign the target for negative edges\n","    edge_label = torch.cat([\n","        train_data.edge_label,\n","        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n","    ], dim=0)\n","    \n","    # make prediction\n","    prediction = model.get_prediction(embedding, edge_label_index).view(-1)\n","    \n","    # optimization\n","    loss = criterion(prediction, edge_label)\n","    loss.backward()\n","    optimizer.step()\n","    return loss"]},{"cell_type":"code","execution_count":157,"id":"ad8d6158","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663389254663,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"ad8d6158"},"outputs":[],"source":["# Implement the test function\n","@torch.no_grad()\n","def test(data):\n","    model.eval()\n","    embedding = model(data.x, data.edge_index)\n","    \n","    # use the sigmoid function to normalize our prediction into [0,1]\n","    out = model.get_prediction(embedding, data.edge_label_index).view(-1).sigmoid()\n","    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())"]},{"cell_type":"code","execution_count":158,"id":"f60a1f15","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2360,"status":"ok","timestamp":1663389257017,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"f60a1f15","outputId":"c3f96ee5-852a-4357-950d-add431c7cd23","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 001, Loss: 0.6932, Val: 0.7057, Test: 0.7006\n","Epoch: 002, Loss: 0.6978, Val: 0.7684, Test: 0.7535\n","Epoch: 003, Loss: 0.6937, Val: 0.6998, Test: 0.6793\n","Epoch: 004, Loss: 0.6934, Val: 0.6203, Test: 0.5770\n","Epoch: 005, Loss: 0.6939, Val: 0.5033, Test: 0.4780\n","Epoch: 006, Loss: 0.6941, Val: 0.5155, Test: 0.5119\n","Epoch: 007, Loss: 0.6940, Val: 0.5662, Test: 0.6166\n","Epoch: 008, Loss: 0.6937, Val: 0.5437, Test: 0.5150\n","Epoch: 009, Loss: 0.6934, Val: 0.5592, Test: 0.5217\n","Epoch: 010, Loss: 0.6933, Val: 0.5363, Test: 0.4930\n","Epoch: 011, Loss: 0.6933, Val: 0.5459, Test: 0.4878\n","Epoch: 012, Loss: 0.6934, Val: 0.5450, Test: 0.4964\n","Epoch: 013, Loss: 0.6935, Val: 0.5458, Test: 0.4991\n","Epoch: 014, Loss: 0.6934, Val: 0.6404, Test: 0.6452\n","Epoch: 015, Loss: 0.6934, Val: 0.6779, Test: 0.6984\n","Epoch: 016, Loss: 0.6932, Val: 0.6882, Test: 0.7129\n","Epoch: 017, Loss: 0.6931, Val: 0.6899, Test: 0.6895\n","Epoch: 018, Loss: 0.6930, Val: 0.7141, Test: 0.7052\n","Epoch: 019, Loss: 0.6929, Val: 0.7157, Test: 0.7301\n","Epoch: 020, Loss: 0.6928, Val: 0.7164, Test: 0.7350\n","Epoch: 021, Loss: 0.6926, Val: 0.7236, Test: 0.7299\n","Epoch: 022, Loss: 0.6924, Val: 0.7292, Test: 0.7262\n","Epoch: 023, Loss: 0.6920, Val: 0.7306, Test: 0.7345\n","Epoch: 024, Loss: 0.6916, Val: 0.7293, Test: 0.7366\n","Epoch: 025, Loss: 0.6910, Val: 0.7283, Test: 0.7319\n","Epoch: 026, Loss: 0.6902, Val: 0.7286, Test: 0.7321\n","Epoch: 027, Loss: 0.6892, Val: 0.7322, Test: 0.7368\n","Epoch: 028, Loss: 0.6881, Val: 0.7347, Test: 0.7386\n","Epoch: 029, Loss: 0.6865, Val: 0.7343, Test: 0.7386\n","Epoch: 030, Loss: 0.6848, Val: 0.7350, Test: 0.7462\n","Epoch: 031, Loss: 0.6827, Val: 0.7376, Test: 0.7477\n","Epoch: 032, Loss: 0.6803, Val: 0.7380, Test: 0.7501\n","Epoch: 033, Loss: 0.6778, Val: 0.7356, Test: 0.7594\n","Epoch: 034, Loss: 0.6750, Val: 0.7402, Test: 0.7645\n","Epoch: 035, Loss: 0.6719, Val: 0.7449, Test: 0.7671\n","Epoch: 036, Loss: 0.6685, Val: 0.7474, Test: 0.7744\n","Epoch: 037, Loss: 0.6644, Val: 0.7511, Test: 0.7785\n","Epoch: 038, Loss: 0.6613, Val: 0.7524, Test: 0.7826\n","Epoch: 039, Loss: 0.6571, Val: 0.7581, Test: 0.7870\n","Epoch: 040, Loss: 0.6542, Val: 0.7636, Test: 0.7917\n","Epoch: 041, Loss: 0.6505, Val: 0.7657, Test: 0.7945\n","Epoch: 042, Loss: 0.6494, Val: 0.7697, Test: 0.7979\n","Epoch: 043, Loss: 0.6488, Val: 0.7784, Test: 0.8047\n","Epoch: 044, Loss: 0.6474, Val: 0.7838, Test: 0.8087\n","Epoch: 045, Loss: 0.6468, Val: 0.7873, Test: 0.8122\n","Epoch: 046, Loss: 0.6415, Val: 0.7940, Test: 0.8181\n","Epoch: 047, Loss: 0.6424, Val: 0.8029, Test: 0.8216\n","Epoch: 048, Loss: 0.6373, Val: 0.8075, Test: 0.8254\n","Epoch: 049, Loss: 0.6379, Val: 0.8100, Test: 0.8277\n","Epoch: 050, Loss: 0.6323, Val: 0.8150, Test: 0.8283\n","Epoch: 051, Loss: 0.6321, Val: 0.8187, Test: 0.8284\n","Epoch: 052, Loss: 0.6319, Val: 0.8183, Test: 0.8305\n","Epoch: 053, Loss: 0.6300, Val: 0.8187, Test: 0.8305\n","Epoch: 054, Loss: 0.6273, Val: 0.8228, Test: 0.8322\n","Epoch: 055, Loss: 0.6269, Val: 0.8231, Test: 0.8310\n","Epoch: 056, Loss: 0.6280, Val: 0.8238, Test: 0.8334\n","Epoch: 057, Loss: 0.6275, Val: 0.8222, Test: 0.8342\n","Epoch: 058, Loss: 0.6199, Val: 0.8233, Test: 0.8338\n","Epoch: 059, Loss: 0.6202, Val: 0.8245, Test: 0.8365\n","Epoch: 060, Loss: 0.6222, Val: 0.8222, Test: 0.8385\n","Epoch: 061, Loss: 0.6160, Val: 0.8218, Test: 0.8383\n","Epoch: 062, Loss: 0.6191, Val: 0.8200, Test: 0.8380\n","Epoch: 063, Loss: 0.6185, Val: 0.8201, Test: 0.8417\n","Epoch: 064, Loss: 0.6175, Val: 0.8170, Test: 0.8421\n","Epoch: 065, Loss: 0.6142, Val: 0.8178, Test: 0.8437\n","Epoch: 066, Loss: 0.6145, Val: 0.8159, Test: 0.8430\n","Epoch: 067, Loss: 0.6129, Val: 0.8159, Test: 0.8453\n","Epoch: 068, Loss: 0.6105, Val: 0.8151, Test: 0.8460\n","Epoch: 069, Loss: 0.6114, Val: 0.8147, Test: 0.8471\n","Epoch: 070, Loss: 0.6113, Val: 0.8156, Test: 0.8477\n","Epoch: 071, Loss: 0.6089, Val: 0.8159, Test: 0.8482\n","Epoch: 072, Loss: 0.6106, Val: 0.8157, Test: 0.8499\n","Epoch: 073, Loss: 0.6110, Val: 0.8153, Test: 0.8499\n","Epoch: 074, Loss: 0.6084, Val: 0.8156, Test: 0.8511\n","Epoch: 075, Loss: 0.6093, Val: 0.8168, Test: 0.8508\n","Epoch: 076, Loss: 0.6044, Val: 0.8179, Test: 0.8513\n","Epoch: 077, Loss: 0.6038, Val: 0.8190, Test: 0.8522\n","Epoch: 078, Loss: 0.6039, Val: 0.8187, Test: 0.8519\n","Epoch: 079, Loss: 0.6064, Val: 0.8202, Test: 0.8522\n","Epoch: 080, Loss: 0.6029, Val: 0.8210, Test: 0.8516\n","Epoch: 081, Loss: 0.6048, Val: 0.8229, Test: 0.8524\n","Epoch: 082, Loss: 0.6027, Val: 0.8224, Test: 0.8514\n","Epoch: 083, Loss: 0.6037, Val: 0.8238, Test: 0.8517\n","Epoch: 084, Loss: 0.6030, Val: 0.8248, Test: 0.8511\n","Epoch: 085, Loss: 0.6034, Val: 0.8243, Test: 0.8504\n","Epoch: 086, Loss: 0.6040, Val: 0.8249, Test: 0.8505\n","Epoch: 087, Loss: 0.5973, Val: 0.8267, Test: 0.8505\n","Epoch: 088, Loss: 0.5999, Val: 0.8278, Test: 0.8501\n","Epoch: 089, Loss: 0.6014, Val: 0.8254, Test: 0.8484\n","Epoch: 090, Loss: 0.5979, Val: 0.8260, Test: 0.8486\n","Epoch: 091, Loss: 0.6004, Val: 0.8284, Test: 0.8496\n","Epoch: 092, Loss: 0.5960, Val: 0.8283, Test: 0.8491\n","Epoch: 093, Loss: 0.6007, Val: 0.8288, Test: 0.8493\n","Epoch: 094, Loss: 0.5974, Val: 0.8271, Test: 0.8474\n","Epoch: 095, Loss: 0.5937, Val: 0.8288, Test: 0.8482\n","Epoch: 096, Loss: 0.5989, Val: 0.8302, Test: 0.8492\n","Epoch: 097, Loss: 0.5908, Val: 0.8281, Test: 0.8483\n","Epoch: 098, Loss: 0.5928, Val: 0.8302, Test: 0.8487\n","Epoch: 099, Loss: 0.5931, Val: 0.8298, Test: 0.8478\n","Epoch: 100, Loss: 0.5966, Val: 0.8276, Test: 0.8466\n","Final Test: 0.8466\n"]}],"source":["best_val_auc = final_test_auc = 0\n","for epoch in range(1, 101):\n","    loss = train()\n","    val_auc = test(val_data)\n","    test_auc = test(test_data)\n","    if val_auc > best_val_auc:\n","        best_val = val_auc\n","        final_test_auc = test_auc\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n","          f'Test: {test_auc:.4f}')\n","\n","print(f'Final Test: {final_test_auc:.4f}')\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":5}
