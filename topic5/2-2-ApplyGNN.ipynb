{"cells":[{"cell_type":"markdown","id":"8f4a91d8","metadata":{"id":"8f4a91d8"},"source":["# Applying GNN Models\n","\n","In this lecture, we will continue using the Cora example from the previous lesson. You will learn about:\n","\n","- Unsupervised GRL\n","- GNNs for Supervised Downstream Tasks\n","\n","We will also compare these methods with the approach discussed in the previous lesson."]},{"cell_type":"code","execution_count":26,"id":"fb43d373","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25952,"status":"ok","timestamp":1663384864744,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"fb43d373","outputId":"836e9449-86ab-48cd-d998-5fd0622d55a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.1.1\n"]}],"source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","# !pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n","\n","import os.path as osp\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.transforms import NormalizeFeatures\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.linear_model import LogisticRegression\n","from torch_geometric.loader import LinkNeighborLoader"]},{"cell_type":"markdown","id":"a9e4160e","metadata":{},"source":["## Unsupervised Graph Representation Learning with GraphSAGE\n","Since we aim to learn graph representations through an unsupervised method, we do not use node labels for training.<br>\n","\n","We assume that if there is a link between a pair of nodes, those nodes should have similar embeddings. Conversely, if there is no link between a pair of nodes, their embeddings should be dissimilar.\n","\n","Based on this assumption, we can define the following loss function:\n"," \n","\\begin{equation}\n","\\text{Loss} = - \\left( \\log \\left( \\sigma(h_u^{\\top} h_v) \\right) - \\sum_{i=1}^k \\log \\left( \\sigma(h_u^{\\top} h_{n_i}) \\right) \\right) , n_i \\sim P_V\n","\\end{equation}\n","\n","- $\\log \\left( \\sigma (h_u^{\\top} h_v) \\right)$:  The similarity between the positive sample pair (i.e., true neighbors). Maximizing this term means you want the similarity of positive samples to be as high as possible.\n","  \n","- $- \\sum_{i=1}^k \\log \\left( \\sigma (h_u^{\\top} h_{n_i}) \\right)$: The similarity between the negative sample pairs (i.e., non-neighbors). Minimizing this term means you want the similarity of negative samples to be as low as possible.\n","\n","Once the embeddings are obtained, they are fed into an additional classifier for node classification."]},{"cell_type":"code","execution_count":27,"id":"YbBjup3U9MQk","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3020,"status":"ok","timestamp":1663386340276,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"YbBjup3U9MQk","outputId":"465f6186-36da-486b-c75f-aceb8876cb54"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","dataset = 'Cora'\n","path = osp.join('.', 'data', dataset)\n","dataset = Planetoid(root=path, name='Cora', transform=NormalizeFeatures())\n","data = dataset[0]\n","print(data)"]},{"cell_type":"code","execution_count":28,"id":"e0c194b0","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663386341904,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"e0c194b0"},"outputs":[],"source":["from torch_geometric.nn import SAGEConv\n","\n","class GraphSAGE(nn.Module):\n","    def __init__(self, in_channels, hidden_channels, num_layers):\n","        super().__init__()\n","        self.num_layers = num_layers\n","        self.convs = nn.ModuleList()\n","        for i in range(num_layers):\n","            in_channels = in_channels if i == 0 else hidden_channels\n","            self.convs.append(SAGEConv(in_channels, hidden_channels))\n","\n","    def forward(self, x, edge_index):\n","        for i, conv in enumerate(self.convs):\n","            x = conv(x, edge_index)\n","            if i != self.num_layers - 1:\n","                x = F.relu(x)\n","                x = F.dropout(x, p=0.5, training=self.training)\n","        return x"]},{"cell_type":"code","execution_count":29,"id":"583902a9","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663386459760,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"583902a9"},"outputs":[],"source":["# define neighbor sampler\n","train_loader = LinkNeighborLoader(\n","    data,\n","    batch_size=256,\n","    shuffle=True,\n","    neg_sampling_ratio=1.0,\n","    num_neighbors=[10, 10],\n",")"]},{"cell_type":"code","execution_count":30,"id":"BK7x8-jo9MQl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1663386460858,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"BK7x8-jo9MQl","outputId":"823cdf0f-de02-47c1-8696-70e42e26b13c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data(x=[2448, 1433], edge_index=[2, 7895], y=[2448], train_mask=[2448], val_mask=[2448], test_mask=[2448], n_id=[2448], e_id=[7895], input_id=[256], edge_label_index=[2, 512], edge_label=[512])\n"]}],"source":["for batch in train_loader:\n","    print(batch)\n","    break"]},{"cell_type":"code","execution_count":31,"id":"kWzD1vPb9MQl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283,"status":"ok","timestamp":1663386488587,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"kWzD1vPb9MQl","outputId":"ae18e461-89fa-43c3-c0af-16f1d6d005a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Edge label index: containing both positive and negative edges\n","tensor([[308, 353, 703,  ..., 162, 654, 258],\n","        [686, 688, 704,  ..., 278, 464, 675]])\n","Edge label: 1 stands for positive and 0 stands for negative node pair(edge)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.])\n"]}],"source":["print(\"Edge label index: containing both positive and negative edges\")\n","print(batch.edge_label_index)\n","\n","print(\"Edge label: 1 stands for positive and 0 stands for negative node pair(edge)\")\n","print(batch.edge_label)"]},{"cell_type":"code","execution_count":32,"id":"b5890c80","metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = GraphSAGE(data.num_node_features, hidden_channels=64, num_layers=2)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n","model = model.to(device)\n","x, edge_index = data.x.to(device), data.edge_index.to(device)"]},{"cell_type":"code","execution_count":33,"id":"BaT6bJq89MQl","metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1663386518319,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"BaT6bJq89MQl"},"outputs":[],"source":["# define training and testing functions\n","def train():\n","    model.train()\n","\n","    total_loss = 0\n","    for batch in train_loader:\n","        batch = batch.to(device)\n","        optimizer.zero_grad()\n","        embedding = model(batch.x, batch.edge_index)\n","        embedding_src = embedding[batch.edge_label_index[0]]\n","        embedding_dst = embedding[batch.edge_label_index[1]]\n","        pred = (embedding_src * embedding_dst).sum(dim=-1)\n","        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += float(loss) * pred.size(0)\n","\n","    return total_loss / data.num_nodes\n","\n","\n","@torch.no_grad()\n","def test():\n","    model.eval()\n","    out = model(data.x.to(device), data.edge_index.to(device)).cpu() \n","\n","    clf = LogisticRegression()\n","    clf.fit(out[data.train_mask], data.y[data.train_mask])\n","\n","    val_acc = clf.score(out[data.val_mask], data.y[data.val_mask])\n","    test_acc = clf.score(out[data.test_mask], data.y[data.test_mask])\n","\n","    return val_acc, test_acc"]},{"cell_type":"code","execution_count":34,"id":"e88903af","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78288,"status":"ok","timestamp":1663386603993,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"e88903af","outputId":"a9284f85-5802-437a-eb7c-c4cba8e390ac","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 001, Loss: 5.1395, Val: 0.3280, Test: 0.3180\n","Epoch: 002, Loss: 4.6441, Val: 0.3380, Test: 0.3070\n","Epoch: 003, Loss: 4.5985, Val: 0.4560, Test: 0.4170\n","Epoch: 004, Loss: 4.4559, Val: 0.5420, Test: 0.5190\n","Epoch: 005, Loss: 4.2841, Val: 0.6020, Test: 0.6090\n","Epoch: 006, Loss: 4.1689, Val: 0.6200, Test: 0.6120\n","Epoch: 007, Loss: 4.0831, Val: 0.6400, Test: 0.6580\n","Epoch: 008, Loss: 3.9265, Val: 0.6580, Test: 0.6820\n","Epoch: 009, Loss: 3.9477, Val: 0.6740, Test: 0.6820\n","Epoch: 010, Loss: 3.8382, Val: 0.6680, Test: 0.6950\n","Epoch: 011, Loss: 3.8586, Val: 0.6780, Test: 0.6930\n","Epoch: 012, Loss: 3.7633, Val: 0.6980, Test: 0.7190\n","Epoch: 013, Loss: 3.7493, Val: 0.6880, Test: 0.7070\n","Epoch: 014, Loss: 3.7360, Val: 0.7060, Test: 0.7170\n","Epoch: 015, Loss: 3.7332, Val: 0.7080, Test: 0.7140\n","Epoch: 016, Loss: 3.6879, Val: 0.7020, Test: 0.7130\n","Epoch: 017, Loss: 3.7057, Val: 0.7080, Test: 0.7380\n","Epoch: 018, Loss: 3.6804, Val: 0.7220, Test: 0.7430\n","Epoch: 019, Loss: 3.6435, Val: 0.7180, Test: 0.7580\n","Epoch: 020, Loss: 3.6595, Val: 0.7240, Test: 0.7370\n","Epoch: 021, Loss: 3.6206, Val: 0.7240, Test: 0.7420\n","Epoch: 022, Loss: 3.6565, Val: 0.7220, Test: 0.7430\n","Epoch: 023, Loss: 3.6136, Val: 0.7420, Test: 0.7430\n","Epoch: 024, Loss: 3.6140, Val: 0.7500, Test: 0.7480\n","Epoch: 025, Loss: 3.6037, Val: 0.7580, Test: 0.7590\n","Epoch: 026, Loss: 3.6344, Val: 0.7480, Test: 0.7770\n","Epoch: 027, Loss: 3.6204, Val: 0.7520, Test: 0.7720\n","Epoch: 028, Loss: 3.5896, Val: 0.7520, Test: 0.7700\n","Epoch: 029, Loss: 3.5994, Val: 0.7460, Test: 0.7760\n","Epoch: 030, Loss: 3.5965, Val: 0.7500, Test: 0.7760\n","Epoch: 031, Loss: 3.5932, Val: 0.7600, Test: 0.7740\n","Epoch: 032, Loss: 3.5722, Val: 0.7500, Test: 0.7700\n","Epoch: 033, Loss: 3.5534, Val: 0.7560, Test: 0.7750\n","Epoch: 034, Loss: 3.5381, Val: 0.7580, Test: 0.7830\n","Epoch: 035, Loss: 3.5731, Val: 0.7700, Test: 0.7800\n","Epoch: 036, Loss: 3.5276, Val: 0.7620, Test: 0.7770\n","Epoch: 037, Loss: 3.5113, Val: 0.7580, Test: 0.7640\n","Epoch: 038, Loss: 3.5581, Val: 0.7460, Test: 0.7750\n","Epoch: 039, Loss: 3.5429, Val: 0.7640, Test: 0.7870\n","Epoch: 040, Loss: 3.5268, Val: 0.7560, Test: 0.7960\n","Epoch: 041, Loss: 3.5809, Val: 0.7480, Test: 0.7850\n","Epoch: 042, Loss: 3.5352, Val: 0.7660, Test: 0.7840\n","Epoch: 043, Loss: 3.5750, Val: 0.7700, Test: 0.7830\n","Epoch: 044, Loss: 3.5075, Val: 0.7780, Test: 0.7910\n","Epoch: 045, Loss: 3.4998, Val: 0.7580, Test: 0.7840\n","Epoch: 046, Loss: 3.5523, Val: 0.7480, Test: 0.7870\n","Epoch: 047, Loss: 3.5490, Val: 0.7660, Test: 0.7820\n","Epoch: 048, Loss: 3.5111, Val: 0.7520, Test: 0.7820\n","Epoch: 049, Loss: 3.5111, Val: 0.7640, Test: 0.7940\n","Epoch: 050, Loss: 3.5380, Val: 0.7600, Test: 0.7900\n","Epoch: 051, Loss: 3.5202, Val: 0.7600, Test: 0.7900\n","Epoch: 052, Loss: 3.5239, Val: 0.7700, Test: 0.7900\n","Epoch: 053, Loss: 3.5148, Val: 0.7680, Test: 0.7860\n","Epoch: 054, Loss: 3.5103, Val: 0.7680, Test: 0.7880\n","Epoch: 055, Loss: 3.5126, Val: 0.7520, Test: 0.7750\n","Epoch: 056, Loss: 3.5206, Val: 0.7360, Test: 0.7550\n","Epoch: 057, Loss: 3.5258, Val: 0.7520, Test: 0.7760\n","Epoch: 058, Loss: 3.4834, Val: 0.7600, Test: 0.7800\n","Epoch: 059, Loss: 3.4877, Val: 0.7760, Test: 0.7840\n","Epoch: 060, Loss: 3.5165, Val: 0.7840, Test: 0.7840\n","Epoch: 061, Loss: 3.5082, Val: 0.7720, Test: 0.7780\n","Epoch: 062, Loss: 3.4682, Val: 0.7640, Test: 0.7840\n","Epoch: 063, Loss: 3.4895, Val: 0.7740, Test: 0.7880\n","Epoch: 064, Loss: 3.5056, Val: 0.7840, Test: 0.7870\n","Epoch: 065, Loss: 3.5579, Val: 0.7720, Test: 0.7990\n","Epoch: 066, Loss: 3.4840, Val: 0.7680, Test: 0.7900\n","Epoch: 067, Loss: 3.5164, Val: 0.7680, Test: 0.8030\n","Epoch: 068, Loss: 3.4996, Val: 0.7720, Test: 0.7840\n","Epoch: 069, Loss: 3.4738, Val: 0.7800, Test: 0.7850\n","Epoch: 070, Loss: 3.4763, Val: 0.7840, Test: 0.7970\n","Epoch: 071, Loss: 3.5095, Val: 0.7740, Test: 0.7860\n","Epoch: 072, Loss: 3.4720, Val: 0.7600, Test: 0.7830\n","Epoch: 073, Loss: 3.4837, Val: 0.7780, Test: 0.7810\n","Epoch: 074, Loss: 3.4942, Val: 0.7920, Test: 0.7790\n","Epoch: 075, Loss: 3.4637, Val: 0.7740, Test: 0.7910\n","Epoch: 076, Loss: 3.4976, Val: 0.7800, Test: 0.7920\n","Epoch: 077, Loss: 3.4821, Val: 0.7800, Test: 0.7890\n","Epoch: 078, Loss: 3.4915, Val: 0.7760, Test: 0.7900\n","Epoch: 079, Loss: 3.4791, Val: 0.7680, Test: 0.7940\n","Epoch: 080, Loss: 3.4692, Val: 0.7800, Test: 0.8140\n","Epoch: 081, Loss: 3.4919, Val: 0.8020, Test: 0.8160\n","Epoch: 082, Loss: 3.4514, Val: 0.7900, Test: 0.7960\n","Epoch: 083, Loss: 3.4757, Val: 0.7880, Test: 0.7820\n","Epoch: 084, Loss: 3.4910, Val: 0.7960, Test: 0.8010\n","Epoch: 085, Loss: 3.4729, Val: 0.7740, Test: 0.8030\n","Epoch: 086, Loss: 3.4910, Val: 0.7720, Test: 0.7920\n","Epoch: 087, Loss: 3.4993, Val: 0.7600, Test: 0.7930\n","Epoch: 088, Loss: 3.4575, Val: 0.7660, Test: 0.7940\n","Epoch: 089, Loss: 3.4571, Val: 0.7720, Test: 0.8030\n","Epoch: 090, Loss: 3.4895, Val: 0.7840, Test: 0.7970\n","Epoch: 091, Loss: 3.4718, Val: 0.7800, Test: 0.7930\n","Epoch: 092, Loss: 3.4706, Val: 0.7800, Test: 0.7890\n","Epoch: 093, Loss: 3.4699, Val: 0.7900, Test: 0.7860\n","Epoch: 094, Loss: 3.4810, Val: 0.7860, Test: 0.7910\n","Epoch: 095, Loss: 3.4659, Val: 0.7720, Test: 0.7910\n","Epoch: 096, Loss: 3.4580, Val: 0.7860, Test: 0.8040\n","Epoch: 097, Loss: 3.4588, Val: 0.7840, Test: 0.8080\n","Epoch: 098, Loss: 3.4603, Val: 0.7740, Test: 0.7990\n","Epoch: 099, Loss: 3.4450, Val: 0.7800, Test: 0.8010\n","Epoch: 100, Loss: 3.4765, Val: 0.7780, Test: 0.8030\n"]}],"source":["for epoch in range(1, 101):\n","    loss = train()\n","    val_acc, test_acc = test()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n","          f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"]},{"cell_type":"markdown","id":"6caee53d","metadata":{"id":"6caee53d"},"source":["## Performance comparison\n","Recall that in the previous examples, we performed node classification in 3 different ways.\n","1. Bag of word + MLP with `Accuracy:0.6`\n","2. Node2vec + Logistic regression with `Accuracy:0.703`\n","3. Node2vec with Bag of word + Logistic regression with `Accuracy:0.707`\n","4. GraphSAGE: with Bag of word + Logistic regression with `Accuracy:0.803`\n","\n","We make use of node feature and graph structure at the same time and boost the accuracy up to **0.791** with a simple two-layer `GraphSAGE`."]},{"cell_type":"markdown","id":"36240400","metadata":{"id":"36240400"},"source":["## End-to-end semi-supervised learning with Graph Convolution Network(GCN)\n","Previously, we adopt a two stage classification pipeline where we first extract network feature via unsupervised learning then utilize a classifier to predict its label. <br>\n","The two-stage design could be suboptimal since the network features were not extracted for specific task. <br>\n","Therefore, we now employ an end-to-end approach. This ensures that the features learned by the model are directly aligned with the specific task, potentially leading to better performance."]},{"cell_type":"code","execution_count":35,"id":"9b156ae0","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1663386768615,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"9b156ae0"},"outputs":[],"source":["from torch_geometric.nn import GCNConv\n","import torch.nn.functional as F\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels):\n","        super().__init__()\n","        self.conv1 = GCNConv(in_channels, hidden_channels, cached=True,\n","                             normalize=True)\n","        self.conv2 = GCNConv(hidden_channels, out_channels, cached=True,\n","                             normalize=True)\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        x = F.dropout(x, p=0.3, training=self.training)\n","        x = self.conv1(x, edge_index, edge_weight).relu()\n","        x = F.dropout(x, p=0.3, training=self.training)\n","        x = self.conv2(x, edge_index, edge_weight)\n","        return x"]},{"cell_type":"code","execution_count":39,"id":"67338214","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663386768902,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"67338214","outputId":"07232e0d-90e6-43ef-e805-859283b424b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["GCN(\n","  (conv1): GCNConv(1433, 64)\n","  (conv2): GCNConv(64, 7)\n",")\n"]}],"source":["dim = 64\n","model = GCN(dataset.num_features, dim, dataset.num_classes)\n","model, data = model.to(device), data.to(device)\n","optimizer = torch.optim.Adam(model.parameters(),weight_decay=1e-4)\n","print(model)"]},{"cell_type":"code","execution_count":40,"id":"646fb96c","metadata":{"executionInfo":{"elapsed":268,"status":"ok","timestamp":1663386794576,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"646fb96c"},"outputs":[],"source":["def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data.x, data.edge_index, data.edge_weight)\n","    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    return float(loss)\n","\n","@torch.no_grad()\n","def test():\n","    model.eval()\n","    pred = model(data.x, data.edge_index, data.edge_weight).argmax(dim=-1)\n","    mask = data.test_mask\n","    accs = (int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n","    return accs"]},{"cell_type":"code","execution_count":41,"id":"142e2595","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2101,"status":"ok","timestamp":1663386797988,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"142e2595","outputId":"9596e72b-144a-43f7-ba6f-078dcc2338e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss:1.9458 Testing accuracy:0.3040\n","Loss:1.9438 Testing accuracy:0.3250\n","Loss:1.9422 Testing accuracy:0.3630\n","Loss:1.9404 Testing accuracy:0.4290\n","Loss:1.9387 Testing accuracy:0.4990\n","Loss:1.9373 Testing accuracy:0.5590\n","Loss:1.9349 Testing accuracy:0.6140\n","Loss:1.9332 Testing accuracy:0.6590\n","Loss:1.9308 Testing accuracy:0.6930\n","Loss:1.9286 Testing accuracy:0.7270\n","Loss:1.9260 Testing accuracy:0.7470\n","Loss:1.9242 Testing accuracy:0.7510\n","Loss:1.9213 Testing accuracy:0.7640\n","Loss:1.9187 Testing accuracy:0.7720\n","Loss:1.9171 Testing accuracy:0.7820\n","Loss:1.9125 Testing accuracy:0.7870\n","Loss:1.9120 Testing accuracy:0.7930\n","Loss:1.9083 Testing accuracy:0.7940\n","Loss:1.9039 Testing accuracy:0.7970\n","Loss:1.9019 Testing accuracy:0.7960\n","Loss:1.9004 Testing accuracy:0.7970\n","Loss:1.8966 Testing accuracy:0.7970\n","Loss:1.8927 Testing accuracy:0.7960\n","Loss:1.8894 Testing accuracy:0.7970\n","Loss:1.8869 Testing accuracy:0.7980\n","Loss:1.8824 Testing accuracy:0.7980\n","Loss:1.8810 Testing accuracy:0.7990\n","Loss:1.8751 Testing accuracy:0.7960\n","Loss:1.8723 Testing accuracy:0.7980\n","Loss:1.8695 Testing accuracy:0.7980\n","Loss:1.8658 Testing accuracy:0.7990\n","Loss:1.8610 Testing accuracy:0.7980\n","Loss:1.8607 Testing accuracy:0.7990\n","Loss:1.8568 Testing accuracy:0.8000\n","Loss:1.8511 Testing accuracy:0.7990\n","Loss:1.8490 Testing accuracy:0.7960\n","Loss:1.8451 Testing accuracy:0.7950\n","Loss:1.8407 Testing accuracy:0.7950\n","Loss:1.8342 Testing accuracy:0.7980\n","Loss:1.8321 Testing accuracy:0.7960\n","Loss:1.8264 Testing accuracy:0.7960\n","Loss:1.8253 Testing accuracy:0.7970\n","Loss:1.8192 Testing accuracy:0.7970\n","Loss:1.8132 Testing accuracy:0.8000\n","Loss:1.8122 Testing accuracy:0.8000\n","Loss:1.8080 Testing accuracy:0.8010\n","Loss:1.8060 Testing accuracy:0.8010\n","Loss:1.7993 Testing accuracy:0.8030\n","Loss:1.7932 Testing accuracy:0.8030\n","Loss:1.7862 Testing accuracy:0.8030\n","Loss:1.7855 Testing accuracy:0.8010\n","Loss:1.7799 Testing accuracy:0.8020\n","Loss:1.7756 Testing accuracy:0.8020\n","Loss:1.7726 Testing accuracy:0.8020\n","Loss:1.7672 Testing accuracy:0.8030\n","Loss:1.7642 Testing accuracy:0.8030\n","Loss:1.7582 Testing accuracy:0.8030\n","Loss:1.7463 Testing accuracy:0.8030\n","Loss:1.7441 Testing accuracy:0.8010\n","Loss:1.7418 Testing accuracy:0.8000\n","Loss:1.7400 Testing accuracy:0.8000\n","Loss:1.7290 Testing accuracy:0.7980\n","Loss:1.7281 Testing accuracy:0.7970\n","Loss:1.7211 Testing accuracy:0.7960\n","Loss:1.7216 Testing accuracy:0.7960\n","Loss:1.7069 Testing accuracy:0.7960\n","Loss:1.7124 Testing accuracy:0.7950\n","Loss:1.7025 Testing accuracy:0.7960\n","Loss:1.6967 Testing accuracy:0.7940\n","Loss:1.6902 Testing accuracy:0.7940\n","Loss:1.6848 Testing accuracy:0.7940\n","Loss:1.6821 Testing accuracy:0.7940\n","Loss:1.6695 Testing accuracy:0.7950\n","Loss:1.6682 Testing accuracy:0.7950\n","Loss:1.6618 Testing accuracy:0.7950\n","Loss:1.6674 Testing accuracy:0.7950\n","Loss:1.6504 Testing accuracy:0.7950\n","Loss:1.6411 Testing accuracy:0.7950\n","Loss:1.6342 Testing accuracy:0.7940\n","Loss:1.6319 Testing accuracy:0.7940\n","Loss:1.6256 Testing accuracy:0.7910\n","Loss:1.6150 Testing accuracy:0.7900\n","Loss:1.6114 Testing accuracy:0.7920\n","Loss:1.6126 Testing accuracy:0.7930\n","Loss:1.6116 Testing accuracy:0.7930\n","Loss:1.6036 Testing accuracy:0.7930\n","Loss:1.5840 Testing accuracy:0.7930\n","Loss:1.5833 Testing accuracy:0.7930\n","Loss:1.5884 Testing accuracy:0.7940\n","Loss:1.5842 Testing accuracy:0.7960\n","Loss:1.5582 Testing accuracy:0.7960\n","Loss:1.5529 Testing accuracy:0.7950\n","Loss:1.5450 Testing accuracy:0.7940\n","Loss:1.5444 Testing accuracy:0.7940\n","Loss:1.5466 Testing accuracy:0.7940\n","Loss:1.5362 Testing accuracy:0.7940\n","Loss:1.5260 Testing accuracy:0.7940\n","Loss:1.5213 Testing accuracy:0.7940\n","Loss:1.5136 Testing accuracy:0.7940\n","Loss:1.5116 Testing accuracy:0.7950\n","Loss:1.4995 Testing accuracy:0.7950\n","Loss:1.4782 Testing accuracy:0.7950\n","Loss:1.4784 Testing accuracy:0.7940\n","Loss:1.4808 Testing accuracy:0.7950\n","Loss:1.4808 Testing accuracy:0.7960\n","Loss:1.4619 Testing accuracy:0.7970\n","Loss:1.4562 Testing accuracy:0.7980\n","Loss:1.4544 Testing accuracy:0.8000\n","Loss:1.4426 Testing accuracy:0.7990\n","Loss:1.4431 Testing accuracy:0.7990\n","Loss:1.4433 Testing accuracy:0.7980\n","Loss:1.4218 Testing accuracy:0.7970\n","Loss:1.4089 Testing accuracy:0.7970\n","Loss:1.4014 Testing accuracy:0.7970\n","Loss:1.3996 Testing accuracy:0.7980\n","Loss:1.3844 Testing accuracy:0.7990\n","Loss:1.3999 Testing accuracy:0.8010\n","Loss:1.3741 Testing accuracy:0.8010\n","Loss:1.3729 Testing accuracy:0.8020\n","Loss:1.3693 Testing accuracy:0.8020\n","Loss:1.3554 Testing accuracy:0.8020\n","Loss:1.3449 Testing accuracy:0.8020\n","Loss:1.3438 Testing accuracy:0.8030\n","Loss:1.3422 Testing accuracy:0.8040\n","Loss:1.3221 Testing accuracy:0.8040\n","Loss:1.3221 Testing accuracy:0.8040\n","Loss:1.3191 Testing accuracy:0.8040\n","Loss:1.3047 Testing accuracy:0.8030\n","Loss:1.2991 Testing accuracy:0.8030\n","Loss:1.3035 Testing accuracy:0.8020\n","Loss:1.2964 Testing accuracy:0.8020\n","Loss:1.2849 Testing accuracy:0.8010\n","Loss:1.2680 Testing accuracy:0.7990\n","Loss:1.2759 Testing accuracy:0.7980\n","Loss:1.2539 Testing accuracy:0.7990\n","Loss:1.2579 Testing accuracy:0.7990\n","Loss:1.2363 Testing accuracy:0.8000\n","Loss:1.2345 Testing accuracy:0.7990\n","Loss:1.2356 Testing accuracy:0.8000\n","Loss:1.2311 Testing accuracy:0.7990\n","Loss:1.2096 Testing accuracy:0.7990\n","Loss:1.2120 Testing accuracy:0.7990\n","Loss:1.2028 Testing accuracy:0.7990\n","Loss:1.1979 Testing accuracy:0.8010\n","Loss:1.2037 Testing accuracy:0.8010\n","Loss:1.1881 Testing accuracy:0.8000\n","Loss:1.1566 Testing accuracy:0.8010\n","Loss:1.1605 Testing accuracy:0.7990\n","Loss:1.1714 Testing accuracy:0.8010\n","Loss:1.1418 Testing accuracy:0.8000\n","Loss:1.1328 Testing accuracy:0.7990\n","Loss:1.1535 Testing accuracy:0.8000\n","Loss:1.1307 Testing accuracy:0.8000\n","Loss:1.1294 Testing accuracy:0.8010\n","Loss:1.1211 Testing accuracy:0.8050\n","Loss:1.1225 Testing accuracy:0.8040\n","Loss:1.0892 Testing accuracy:0.8050\n","Loss:1.1046 Testing accuracy:0.8050\n","Loss:1.0956 Testing accuracy:0.8040\n","Loss:1.0625 Testing accuracy:0.8050\n","Loss:1.0824 Testing accuracy:0.8050\n","Loss:1.0676 Testing accuracy:0.8050\n","Loss:1.0513 Testing accuracy:0.8040\n","Loss:1.0676 Testing accuracy:0.8050\n","Loss:1.0589 Testing accuracy:0.8060\n","Loss:1.0515 Testing accuracy:0.8060\n","Loss:1.0472 Testing accuracy:0.8060\n","Loss:1.0259 Testing accuracy:0.8060\n","Loss:1.0239 Testing accuracy:0.8050\n","Loss:1.0205 Testing accuracy:0.8040\n","Loss:1.0084 Testing accuracy:0.8060\n","Loss:0.9978 Testing accuracy:0.8060\n","Loss:0.9911 Testing accuracy:0.8050\n","Loss:0.9853 Testing accuracy:0.8070\n","Loss:0.9688 Testing accuracy:0.8080\n","Loss:0.9795 Testing accuracy:0.8080\n","Loss:0.9552 Testing accuracy:0.8080\n","Loss:0.9591 Testing accuracy:0.8090\n","Loss:0.9364 Testing accuracy:0.8090\n","Loss:0.9511 Testing accuracy:0.8090\n","Loss:0.9472 Testing accuracy:0.8090\n","Loss:0.9243 Testing accuracy:0.8070\n","Loss:0.9064 Testing accuracy:0.8070\n","Loss:0.9299 Testing accuracy:0.8040\n","Loss:0.9331 Testing accuracy:0.8030\n","Loss:0.9211 Testing accuracy:0.8030\n","Loss:0.9122 Testing accuracy:0.8030\n","Loss:0.9138 Testing accuracy:0.8040\n","Loss:0.8862 Testing accuracy:0.8030\n","Loss:0.9082 Testing accuracy:0.8040\n","Loss:0.8929 Testing accuracy:0.8090\n","Loss:0.8677 Testing accuracy:0.8090\n","Loss:0.8810 Testing accuracy:0.8090\n","Loss:0.8653 Testing accuracy:0.8100\n","Loss:0.8415 Testing accuracy:0.8130\n","Loss:0.8312 Testing accuracy:0.8130\n","Loss:0.8451 Testing accuracy:0.8130\n","Loss:0.8301 Testing accuracy:0.8150\n","Loss:0.8333 Testing accuracy:0.8150\n","Loss:0.8568 Testing accuracy:0.8180\n"]}],"source":["for epoch in range(200):\n","    loss = train()\n","    test_acc = test()\n","    print(f\"Loss:{loss:.4f} Testing accuracy:{test_acc:.4f}\")"]},{"cell_type":"markdown","id":"ec8ff377","metadata":{"id":"ec8ff377"},"source":["## Performance comparison\n","Recall that in the previous examples, we performed node classification in 3 different ways.\n","1. Bag of word + MLP with `Accuracy:0.6`\n","2. Node2vec + Logistic regression with `Accuracy:0.703`\n","3. Node2vec with Bag of word + Logistic regression with `Accuracy:0.707`\n","4. GraphSAGE: with Bag of word + Logistic regression with `Accuracy:0.803`\n","4. GCN: end-to-end learning with `Accuracy:0.818`\n","\n","From this example, we clearly figure out that using GCN with end2end training leads to the best performance since the feature extracted could be learned to optimize the node classification task."]},{"cell_type":"markdown","id":"d347b13c","metadata":{"id":"d347b13c"},"source":["## Applying different GNN backbone layer\n","The full list of implemented GNN could be found in [here.](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers)"]},{"cell_type":"code","execution_count":15,"id":"765f7d34","metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1663387016492,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"765f7d34"},"outputs":[],"source":["from torch_geometric.nn import GCNConv,GATConv, GraphSAGE\n","import torch.nn.functional as F\n","\n","class GNN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, gnn_type):\n","        super().__init__()\n","        if gnn_type == \"GCN\":\n","            self.GNN = GCNConv\n","        elif gnn_type == \"SAGE\":\n","            self.GNN = GraphSAGE\n","        elif gnn_type == \"GAT\":\n","            self.GNN = GATConv\n","        \n","        self.conv1 = self.GNN(in_channels, hidden_channels)\n","        self.conv2 = self.GNN(hidden_channels, out_channels)\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv1(x, edge_index, edge_weight).relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index, edge_weight)\n","        return x"]},{"cell_type":"code","execution_count":16,"id":"f37bd755","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1663387016912,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"f37bd755","outputId":"7c33b9d5-4dd0-4305-ca6e-c1bee2a074d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["GNN(\n","  (conv1): GATConv(1433, 32, heads=1)\n","  (conv2): GATConv(32, 7, heads=1)\n",")\n"]}],"source":["dim = 32\n","gnn_type = \"GAT\"\n","model = GNN(dataset.num_features, dim, dataset.num_classes,gnn_type=gnn_type)\n","model, data = model.to(device), data.to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","print(model)"]},{"cell_type":"markdown","id":"afe00d31","metadata":{"id":"afe00d31"},"source":["# HW: Link prediction with GNN\n","1. Try different GNN layer\n","2. Try to optimize the performance by stacking multiple layers\n","3. Report the best accuracy on testing set and the best model configuration(e.g., how many layers?)"]},{"cell_type":"code","execution_count":17,"id":"cfb91924","metadata":{"executionInfo":{"elapsed":281,"status":"ok","timestamp":1663387050805,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"cfb91924"},"outputs":[],"source":["# Let's practice how to use GNN for link prediction\n","# First we need to load the Cora dataset\n","\n","from sklearn.metrics import roc_auc_score\n","import torch_geometric.transforms as T\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.utils import negative_sampling\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","transform = T.Compose([\n","    T.NormalizeFeatures(),\n","    T.ToDevice(device),\n","    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n","                      add_negative_train_samples=True),\n","])\n","dataset = Planetoid(path, name='Cora', transform=transform)\n","train_data, val_data, test_data = dataset[0]"]},{"cell_type":"code","execution_count":18,"id":"43bed8c6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1663387060011,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"43bed8c6","outputId":"24659c22-d453-4534-db45-9d742e94c9b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------Training data------\n","Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[8976], edge_label_index=[2, 8976])\n","Training edges:\n","tensor([[1282,  353, 1986,  ..., 2699, 1780, 2192],\n","        [1483, 1173, 2009,  ...,  506, 1334,  914]])\n","Labels\n","tensor([1., 1., 1.,  ..., 0., 0., 0.])\n","\n","--------Testing data------\n","Data(x=[2708, 1433], edge_index=[2, 9502], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054])\n","Testing edges:\n","tensor([[ 793,  225,  103,  ..., 2237, 1341, 2178],\n","        [1896, 2255,  484,  ...,  685,  192, 1389]])\n","Labels\n","tensor([1., 1., 1.,  ..., 0., 0., 0.])\n"]}],"source":["print(\"--------Training data------\")\n","print(train_data)\n","print(\"Training edges:\")\n","print(train_data.edge_label_index)\n","print(\"Labels\")\n","print(train_data.edge_label)\n","\n","print()\n","print(\"--------Testing data------\")\n","print(test_data)\n","print(\"Testing edges:\")\n","print(test_data.edge_label_index)\n","print(\"Labels\")\n","print(test_data.edge_label)"]},{"cell_type":"code","execution_count":19,"id":"8ccc3487","metadata":{"executionInfo":{"elapsed":1021,"status":"ok","timestamp":1663389253599,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"8ccc3487"},"outputs":[],"source":["class MyGNN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels):\n","        super().__init__()\n","        ############################################################################\n","        # TODO: Your code here! \n","        # create you GNN layer here. \n","        # try to use different GNN backbone layer or stacking multiple layer to boost performance\n","        self.conv1 = GCNConv(in_channels, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, out_channels)\n","        \n","        ############################################################################\n","\n","    def forward(self, x, edge_index):\n","        ############################################################################\n","        # TODO: Your code here! \n","        # Apply the forward pass according to your GNN layers\n","        # you shoud return the embedding of each node (x has shape [num_nodes, dim])    \n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = self.conv2(x, edge_index)  \n","        ############################################################################\n","        return x\n","    \n","    def get_prediction(self, node_embedding, edges):\n","        # In this function, we have the node embedding and edges as input\n","        # Input shapes:\n","        #      node_embedding: (|V|, out_channels)\n","        #      edges: (2, number of edges)\n","        # To generate such output, we use the inner product of embeddings of two nodes\n","        # The output is to generate a scalar for each pair of edge\n","        embedding_first_node = node_embedding[edges[0]]\n","        embedding_second_node = node_embedding[edges[1]]\n","        ############################################################################\n","        # TODO: Your code here! \n","        # implement the element-wise product as edge feature for link prediction\n","        inner_product = torch.sum(embedding_first_node * embedding_second_node, dim=-1)\n","        \n","        ############################################################################\n","        return inner_product"]},{"cell_type":"code","execution_count":20,"id":"8236b1d0","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663389254662,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"8236b1d0"},"outputs":[],"source":["############################################################################\n","# TODO: Your code here! \n","# initiate your GNN model and select the criterion for link prediction\n","\n","model = MyGNN(dataset.num_features, 128, 64).to(device)\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","############################################################################"]},{"cell_type":"code","execution_count":21,"id":"52305879","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663389254663,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"52305879"},"outputs":[],"source":["# Implement the train function\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    embedding = model(train_data.x, train_data.edge_index)\n","\n","    # We perform a new round of negative sampling for every training epoch:\n","    neg_edge_index = negative_sampling(\n","        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n","        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n","\n","    edge_label_index = torch.cat(\n","        [train_data.edge_label_index, neg_edge_index],\n","        dim=-1,\n","    )\n","    \n","    # Please assign the target for negative edges\n","    edge_label = torch.cat([\n","        train_data.edge_label,\n","        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n","    ], dim=0)\n","    \n","    # make prediction\n","    prediction = model.get_prediction(embedding, edge_label_index).view(-1)\n","    \n","    # optimization\n","    loss = criterion(prediction, edge_label)\n","    loss.backward()\n","    optimizer.step()\n","    return loss"]},{"cell_type":"code","execution_count":22,"id":"ad8d6158","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663389254663,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"ad8d6158"},"outputs":[],"source":["# Implement the test function\n","@torch.no_grad()\n","def test(data):\n","    model.eval()\n","    embedding = model(data.x, data.edge_index)\n","    \n","    # use the sigmoid function to normalize our prediction into [0,1]\n","    out = model.get_prediction(embedding, data.edge_label_index).view(-1).sigmoid()\n","    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())"]},{"cell_type":"code","execution_count":23,"id":"f60a1f15","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2360,"status":"ok","timestamp":1663389257017,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"f60a1f15","outputId":"c3f96ee5-852a-4357-950d-add431c7cd23","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 001, Loss: 0.6932, Val: 0.6935, Test: 0.7040\n","Epoch: 002, Loss: 0.7008, Val: 0.7408, Test: 0.7591\n","Epoch: 003, Loss: 0.6936, Val: 0.6889, Test: 0.6780\n","Epoch: 004, Loss: 0.6934, Val: 0.5864, Test: 0.5652\n","Epoch: 005, Loss: 0.6940, Val: 0.6402, Test: 0.5956\n","Epoch: 006, Loss: 0.6944, Val: 0.6259, Test: 0.6506\n","Epoch: 007, Loss: 0.6943, Val: 0.6932, Test: 0.6671\n","Epoch: 008, Loss: 0.6939, Val: 0.6657, Test: 0.6748\n","Epoch: 009, Loss: 0.6935, Val: 0.6579, Test: 0.6609\n","Epoch: 010, Loss: 0.6933, Val: 0.6424, Test: 0.6254\n","Epoch: 011, Loss: 0.6933, Val: 0.6070, Test: 0.5935\n","Epoch: 012, Loss: 0.6933, Val: 0.4723, Test: 0.5012\n","Epoch: 013, Loss: 0.6934, Val: 0.4615, Test: 0.4950\n","Epoch: 014, Loss: 0.6934, Val: 0.4647, Test: 0.4978\n","Epoch: 015, Loss: 0.6933, Val: 0.5046, Test: 0.5264\n","Epoch: 016, Loss: 0.6933, Val: 0.6678, Test: 0.6663\n","Epoch: 017, Loss: 0.6931, Val: 0.7031, Test: 0.7098\n","Epoch: 018, Loss: 0.6930, Val: 0.7287, Test: 0.7264\n","Epoch: 019, Loss: 0.6928, Val: 0.7227, Test: 0.7138\n","Epoch: 020, Loss: 0.6925, Val: 0.7314, Test: 0.7135\n","Epoch: 021, Loss: 0.6923, Val: 0.7186, Test: 0.6981\n","Epoch: 022, Loss: 0.6918, Val: 0.7110, Test: 0.6915\n","Epoch: 023, Loss: 0.6914, Val: 0.6985, Test: 0.6826\n","Epoch: 024, Loss: 0.6904, Val: 0.6976, Test: 0.6765\n","Epoch: 025, Loss: 0.6891, Val: 0.6969, Test: 0.6758\n","Epoch: 026, Loss: 0.6879, Val: 0.7129, Test: 0.6786\n","Epoch: 027, Loss: 0.6861, Val: 0.7106, Test: 0.6778\n","Epoch: 028, Loss: 0.6842, Val: 0.7193, Test: 0.6791\n","Epoch: 029, Loss: 0.6816, Val: 0.7311, Test: 0.6848\n","Epoch: 030, Loss: 0.6778, Val: 0.7452, Test: 0.6966\n","Epoch: 031, Loss: 0.6755, Val: 0.7499, Test: 0.7005\n","Epoch: 032, Loss: 0.6733, Val: 0.7574, Test: 0.7123\n","Epoch: 033, Loss: 0.6743, Val: 0.7626, Test: 0.7183\n","Epoch: 034, Loss: 0.6702, Val: 0.7639, Test: 0.7231\n","Epoch: 035, Loss: 0.6691, Val: 0.7687, Test: 0.7282\n","Epoch: 036, Loss: 0.6674, Val: 0.7725, Test: 0.7333\n","Epoch: 037, Loss: 0.6689, Val: 0.7780, Test: 0.7411\n","Epoch: 038, Loss: 0.6663, Val: 0.7825, Test: 0.7474\n","Epoch: 039, Loss: 0.6580, Val: 0.7872, Test: 0.7548\n","Epoch: 040, Loss: 0.6588, Val: 0.7944, Test: 0.7617\n","Epoch: 041, Loss: 0.6559, Val: 0.7998, Test: 0.7681\n","Epoch: 042, Loss: 0.6534, Val: 0.8055, Test: 0.7731\n","Epoch: 043, Loss: 0.6524, Val: 0.8105, Test: 0.7787\n","Epoch: 044, Loss: 0.6511, Val: 0.8156, Test: 0.7838\n","Epoch: 045, Loss: 0.6483, Val: 0.8187, Test: 0.7874\n","Epoch: 046, Loss: 0.6448, Val: 0.8233, Test: 0.7907\n","Epoch: 047, Loss: 0.6461, Val: 0.8229, Test: 0.7916\n","Epoch: 048, Loss: 0.6443, Val: 0.8324, Test: 0.7975\n","Epoch: 049, Loss: 0.6421, Val: 0.8306, Test: 0.7982\n","Epoch: 050, Loss: 0.6381, Val: 0.8315, Test: 0.7994\n","Epoch: 051, Loss: 0.6364, Val: 0.8405, Test: 0.8076\n","Epoch: 052, Loss: 0.6362, Val: 0.8439, Test: 0.8112\n","Epoch: 053, Loss: 0.6342, Val: 0.8436, Test: 0.8108\n","Epoch: 054, Loss: 0.6287, Val: 0.8485, Test: 0.8145\n","Epoch: 055, Loss: 0.6261, Val: 0.8546, Test: 0.8186\n","Epoch: 056, Loss: 0.6279, Val: 0.8582, Test: 0.8202\n","Epoch: 057, Loss: 0.6289, Val: 0.8573, Test: 0.8176\n","Epoch: 058, Loss: 0.6260, Val: 0.8633, Test: 0.8221\n","Epoch: 059, Loss: 0.6262, Val: 0.8660, Test: 0.8230\n","Epoch: 060, Loss: 0.6302, Val: 0.8675, Test: 0.8226\n","Epoch: 061, Loss: 0.6251, Val: 0.8661, Test: 0.8204\n","Epoch: 062, Loss: 0.6258, Val: 0.8697, Test: 0.8231\n","Epoch: 063, Loss: 0.6220, Val: 0.8714, Test: 0.8236\n","Epoch: 064, Loss: 0.6187, Val: 0.8710, Test: 0.8251\n","Epoch: 065, Loss: 0.6203, Val: 0.8689, Test: 0.8232\n","Epoch: 066, Loss: 0.6184, Val: 0.8713, Test: 0.8241\n","Epoch: 067, Loss: 0.6185, Val: 0.8734, Test: 0.8269\n","Epoch: 068, Loss: 0.6155, Val: 0.8715, Test: 0.8277\n","Epoch: 069, Loss: 0.6154, Val: 0.8720, Test: 0.8277\n","Epoch: 070, Loss: 0.6093, Val: 0.8730, Test: 0.8272\n","Epoch: 071, Loss: 0.6107, Val: 0.8753, Test: 0.8291\n","Epoch: 072, Loss: 0.6106, Val: 0.8758, Test: 0.8312\n","Epoch: 073, Loss: 0.6074, Val: 0.8765, Test: 0.8317\n","Epoch: 074, Loss: 0.6079, Val: 0.8795, Test: 0.8324\n","Epoch: 075, Loss: 0.6039, Val: 0.8801, Test: 0.8336\n","Epoch: 076, Loss: 0.6038, Val: 0.8797, Test: 0.8355\n","Epoch: 077, Loss: 0.6070, Val: 0.8803, Test: 0.8386\n","Epoch: 078, Loss: 0.6016, Val: 0.8809, Test: 0.8388\n","Epoch: 079, Loss: 0.6007, Val: 0.8812, Test: 0.8381\n","Epoch: 080, Loss: 0.6003, Val: 0.8831, Test: 0.8415\n","Epoch: 081, Loss: 0.6031, Val: 0.8798, Test: 0.8420\n","Epoch: 082, Loss: 0.5998, Val: 0.8800, Test: 0.8423\n","Epoch: 083, Loss: 0.5971, Val: 0.8819, Test: 0.8441\n","Epoch: 084, Loss: 0.5971, Val: 0.8829, Test: 0.8440\n","Epoch: 085, Loss: 0.5962, Val: 0.8817, Test: 0.8441\n","Epoch: 086, Loss: 0.5914, Val: 0.8818, Test: 0.8445\n","Epoch: 087, Loss: 0.5953, Val: 0.8823, Test: 0.8452\n","Epoch: 088, Loss: 0.5927, Val: 0.8825, Test: 0.8455\n","Epoch: 089, Loss: 0.5937, Val: 0.8829, Test: 0.8448\n","Epoch: 090, Loss: 0.5932, Val: 0.8825, Test: 0.8459\n","Epoch: 091, Loss: 0.5855, Val: 0.8825, Test: 0.8456\n","Epoch: 092, Loss: 0.5909, Val: 0.8823, Test: 0.8452\n","Epoch: 093, Loss: 0.5893, Val: 0.8821, Test: 0.8453\n","Epoch: 094, Loss: 0.5897, Val: 0.8829, Test: 0.8446\n","Epoch: 095, Loss: 0.5869, Val: 0.8836, Test: 0.8449\n","Epoch: 096, Loss: 0.5902, Val: 0.8833, Test: 0.8458\n","Epoch: 097, Loss: 0.5847, Val: 0.8835, Test: 0.8447\n","Epoch: 098, Loss: 0.5841, Val: 0.8831, Test: 0.8459\n","Epoch: 099, Loss: 0.5897, Val: 0.8836, Test: 0.8456\n","Epoch: 100, Loss: 0.5826, Val: 0.8833, Test: 0.8451\n","Final Test: 0.8451\n"]}],"source":["best_val_auc = final_test_auc = 0\n","for epoch in range(1, 101):\n","    loss = train()\n","    val_auc = test(val_data)\n","    test_auc = test(test_data)\n","    if val_auc > best_val_auc:\n","        best_val = val_auc\n","        final_test_auc = test_auc\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n","          f'Test: {test_auc:.4f}')\n","\n","print(f'Final Test: {final_test_auc:.4f}')\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
