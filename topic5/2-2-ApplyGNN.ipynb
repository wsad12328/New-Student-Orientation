{"cells":[{"cell_type":"markdown","id":"8f4a91d8","metadata":{"id":"8f4a91d8"},"source":["# Applying GNN Models\n","\n","In this lecture, we will continue using the Cora example from the previous lesson. You will learn about:\n","\n","- Unsupervised GRL\n","- GNNs for Supervised Downstream Tasks\n","\n","We will also compare these methods with the approach discussed in the previous lesson."]},{"cell_type":"code","execution_count":1,"id":"fb43d373","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25952,"status":"ok","timestamp":1663384864744,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"fb43d373","outputId":"836e9449-86ab-48cd-d998-5fd0622d55a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.3.0\n"]}],"source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","# !pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n","\n","import os.path as osp\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.transforms import NormalizeFeatures\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.linear_model import LogisticRegression\n","from torch_geometric.loader import LinkNeighborLoader"]},{"cell_type":"markdown","id":"a9e4160e","metadata":{},"source":["## Unsupervised Graph Representation Learning with GraphSAGE\n","Since we aim to learn graph representations through an unsupervised method, we do not use node labels for training.<br>\n","\n","We assume that if there is a link between a pair of nodes, those nodes should have similar embeddings. Conversely, if there is no link between a pair of nodes, their embeddings should be dissimilar.\n","\n","Based on this assumption, we can define the following loss function:\n"," \n","\\begin{equation}\n","\\text{Loss} = - \\left( \\log \\left( \\sigma(h_u^{\\top} h_v) \\right) - \\sum_{i=1}^k \\log \\left( \\sigma(h_u^{\\top} h_{n_i}) \\right) \\right) , n_i \\sim P_V\n","\\end{equation}\n","\n","- $\\log \\left( \\sigma (h_u^{\\top} h_v) \\right)$:  The similarity between the positive sample pair (i.e., true neighbors). Maximizing this term means you want the similarity of positive samples to be as high as possible.\n","  \n","- $- \\sum_{i=1}^k \\log \\left( \\sigma (h_u^{\\top} h_{n_i}) \\right)$: The similarity between the negative sample pairs (i.e., non-neighbors). Minimizing this term means you want the similarity of negative samples to be as low as possible.\n","\n","Once the embeddings are obtained, they are fed into an additional classifier for node classification."]},{"cell_type":"code","execution_count":2,"id":"YbBjup3U9MQk","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3020,"status":"ok","timestamp":1663386340276,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"YbBjup3U9MQk","outputId":"465f6186-36da-486b-c75f-aceb8876cb54"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","dataset = 'Cora'\n","path = osp.join('.', 'data', dataset)\n","dataset = Planetoid(root=path, name='Cora', transform=NormalizeFeatures())\n","data = dataset[0]\n","print(data)"]},{"cell_type":"code","execution_count":3,"id":"e0c194b0","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663386341904,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"e0c194b0"},"outputs":[],"source":["from torch_geometric.nn import SAGEConv\n","\n","class GraphSAGE(nn.Module):\n","    def __init__(self, in_channels, hidden_channels, num_layers):\n","        super().__init__()\n","        self.num_layers = num_layers\n","        self.convs = nn.ModuleList()\n","        for i in range(num_layers):\n","            in_channels = in_channels if i == 0 else hidden_channels\n","            self.convs.append(SAGEConv(in_channels, hidden_channels))\n","\n","    def forward(self, x, edge_index):\n","        for i, conv in enumerate(self.convs):\n","            x = conv(x, edge_index)\n","            if i != self.num_layers - 1:\n","                x = F.relu(x)\n","                x = F.dropout(x, p=0.5, training=self.training)\n","        return x"]},{"cell_type":"code","execution_count":4,"id":"583902a9","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663386459760,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"583902a9"},"outputs":[],"source":["# define neighbor sampler\n","train_loader = LinkNeighborLoader(\n","    data,\n","    batch_size=256,\n","    shuffle=True,\n","    neg_sampling_ratio=1.0,\n","    num_neighbors=[10, 10],\n",")"]},{"cell_type":"code","execution_count":5,"id":"BK7x8-jo9MQl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1663386460858,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"BK7x8-jo9MQl","outputId":"823cdf0f-de02-47c1-8696-70e42e26b13c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data(x=[2427, 1433], edge_index=[2, 7971], y=[2427], train_mask=[2427], val_mask=[2427], test_mask=[2427], n_id=[2427], e_id=[7971], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[256], edge_label_index=[2, 512], edge_label=[512])\n"]}],"source":["for batch in train_loader:\n","    print(batch)\n","    break"]},{"cell_type":"code","execution_count":6,"id":"kWzD1vPb9MQl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283,"status":"ok","timestamp":1663386488587,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"kWzD1vPb9MQl","outputId":"ae18e461-89fa-43c3-c0af-16f1d6d005a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Edge label index: containing both positive and negative edges\n","tensor([[152, 375, 228,  ..., 377, 495, 683],\n","        [394, 739, 698,  ..., 336, 650, 794]])\n","Edge label: 1 stands for positive and 0 stands for negative node pair(edge)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.])\n"]}],"source":["print(\"Edge label index: containing both positive and negative edges\")\n","print(batch.edge_label_index)\n","\n","print(\"Edge label: 1 stands for positive and 0 stands for negative node pair(edge)\")\n","print(batch.edge_label)"]},{"cell_type":"code","execution_count":7,"id":"b5890c80","metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = GraphSAGE(data.num_node_features, hidden_channels=64, num_layers=2)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n","model = model.to(device)\n","x, edge_index = data.x.to(device), data.edge_index.to(device)"]},{"cell_type":"code","execution_count":8,"id":"BaT6bJq89MQl","metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1663386518319,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"BaT6bJq89MQl"},"outputs":[],"source":["# define training and testing functions\n","def train():\n","    model.train()\n","\n","    total_loss = 0\n","    for batch in train_loader:\n","        batch = batch.to(device)\n","        optimizer.zero_grad()\n","        embedding = model(batch.x, batch.edge_index)\n","        embedding_src = embedding[batch.edge_label_index[0]]\n","        embedding_dst = embedding[batch.edge_label_index[1]]\n","        pred = (embedding_src * embedding_dst).sum(dim=-1)\n","        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += float(loss) * pred.size(0)\n","\n","    return total_loss / data.num_nodes\n","\n","\n","@torch.no_grad()\n","def test():\n","    model.eval()\n","    out = model(data.x.to(device), data.edge_index.to(device)).cpu() \n","\n","    clf = LogisticRegression()\n","    clf.fit(out[data.train_mask], data.y[data.train_mask])\n","\n","    val_acc = clf.score(out[data.val_mask], data.y[data.val_mask])\n","    test_acc = clf.score(out[data.test_mask], data.y[data.test_mask])\n","\n","    return val_acc, test_acc"]},{"cell_type":"code","execution_count":9,"id":"e88903af","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78288,"status":"ok","timestamp":1663386603993,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"e88903af","outputId":"a9284f85-5802-437a-eb7c-c4cba8e390ac","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 001, Loss: 5.1687, Val: 0.3480, Test: 0.3140\n","Epoch: 002, Loss: 4.6655, Val: 0.4300, Test: 0.4080\n","Epoch: 003, Loss: 4.5153, Val: 0.4480, Test: 0.4540\n","Epoch: 004, Loss: 4.4635, Val: 0.6300, Test: 0.6110\n","Epoch: 005, Loss: 4.1887, Val: 0.6700, Test: 0.6590\n","Epoch: 006, Loss: 3.9948, Val: 0.6840, Test: 0.6680\n","Epoch: 007, Loss: 3.9501, Val: 0.6880, Test: 0.6910\n","Epoch: 008, Loss: 3.8574, Val: 0.6980, Test: 0.7070\n","Epoch: 009, Loss: 3.8513, Val: 0.7140, Test: 0.7330\n","Epoch: 010, Loss: 3.8072, Val: 0.6960, Test: 0.7230\n","Epoch: 011, Loss: 3.8090, Val: 0.6940, Test: 0.7220\n","Epoch: 012, Loss: 3.7499, Val: 0.7160, Test: 0.7350\n","Epoch: 013, Loss: 3.6991, Val: 0.7020, Test: 0.7370\n","Epoch: 014, Loss: 3.7330, Val: 0.7060, Test: 0.7420\n","Epoch: 015, Loss: 3.6669, Val: 0.7060, Test: 0.7350\n","Epoch: 016, Loss: 3.6889, Val: 0.7160, Test: 0.7390\n","Epoch: 017, Loss: 3.6774, Val: 0.7120, Test: 0.7370\n","Epoch: 018, Loss: 3.6493, Val: 0.7060, Test: 0.7320\n","Epoch: 019, Loss: 3.6760, Val: 0.7000, Test: 0.7360\n","Epoch: 020, Loss: 3.6614, Val: 0.7100, Test: 0.7490\n","Epoch: 021, Loss: 3.6185, Val: 0.6980, Test: 0.7520\n","Epoch: 022, Loss: 3.6251, Val: 0.7180, Test: 0.7610\n","Epoch: 023, Loss: 3.6190, Val: 0.7420, Test: 0.7560\n","Epoch: 024, Loss: 3.6606, Val: 0.7280, Test: 0.7560\n","Epoch: 025, Loss: 3.6341, Val: 0.7360, Test: 0.7690\n","Epoch: 026, Loss: 3.5908, Val: 0.7360, Test: 0.7730\n","Epoch: 027, Loss: 3.5895, Val: 0.7560, Test: 0.7720\n","Epoch: 028, Loss: 3.6022, Val: 0.7580, Test: 0.7880\n","Epoch: 029, Loss: 3.5763, Val: 0.7600, Test: 0.7790\n","Epoch: 030, Loss: 3.6097, Val: 0.7440, Test: 0.7940\n","Epoch: 031, Loss: 3.5971, Val: 0.7400, Test: 0.7730\n","Epoch: 032, Loss: 3.5568, Val: 0.7380, Test: 0.7750\n","Epoch: 033, Loss: 3.6112, Val: 0.7540, Test: 0.7890\n","Epoch: 034, Loss: 3.6036, Val: 0.7480, Test: 0.7860\n","Epoch: 035, Loss: 3.6200, Val: 0.7380, Test: 0.7760\n","Epoch: 036, Loss: 3.5561, Val: 0.7360, Test: 0.7800\n","Epoch: 037, Loss: 3.5538, Val: 0.7400, Test: 0.7780\n","Epoch: 038, Loss: 3.5461, Val: 0.7540, Test: 0.7880\n","Epoch: 039, Loss: 3.5490, Val: 0.7440, Test: 0.7830\n","Epoch: 040, Loss: 3.5515, Val: 0.7340, Test: 0.7780\n","Epoch: 041, Loss: 3.5106, Val: 0.7520, Test: 0.7860\n","Epoch: 042, Loss: 3.5547, Val: 0.7480, Test: 0.7860\n","Epoch: 043, Loss: 3.5654, Val: 0.7480, Test: 0.7830\n","Epoch: 044, Loss: 3.5586, Val: 0.7560, Test: 0.7870\n","Epoch: 045, Loss: 3.4911, Val: 0.7580, Test: 0.7850\n","Epoch: 046, Loss: 3.5337, Val: 0.7580, Test: 0.7860\n","Epoch: 047, Loss: 3.5600, Val: 0.7660, Test: 0.7790\n","Epoch: 048, Loss: 3.5690, Val: 0.7620, Test: 0.7920\n","Epoch: 049, Loss: 3.5563, Val: 0.7740, Test: 0.7850\n","Epoch: 050, Loss: 3.4802, Val: 0.7720, Test: 0.7830\n","Epoch: 051, Loss: 3.4900, Val: 0.7660, Test: 0.7830\n","Epoch: 052, Loss: 3.5167, Val: 0.7880, Test: 0.7920\n","Epoch: 053, Loss: 3.4850, Val: 0.7840, Test: 0.7890\n","Epoch: 054, Loss: 3.5456, Val: 0.7720, Test: 0.7870\n","Epoch: 055, Loss: 3.5500, Val: 0.7960, Test: 0.7950\n","Epoch: 056, Loss: 3.5458, Val: 0.7820, Test: 0.7900\n","Epoch: 057, Loss: 3.5150, Val: 0.7860, Test: 0.7920\n","Epoch: 058, Loss: 3.5240, Val: 0.7840, Test: 0.7830\n","Epoch: 059, Loss: 3.5227, Val: 0.7740, Test: 0.7820\n","Epoch: 060, Loss: 3.4750, Val: 0.7900, Test: 0.7880\n","Epoch: 061, Loss: 3.5269, Val: 0.7820, Test: 0.8070\n","Epoch: 062, Loss: 3.4979, Val: 0.7760, Test: 0.8060\n","Epoch: 063, Loss: 3.5206, Val: 0.7720, Test: 0.7930\n","Epoch: 064, Loss: 3.5090, Val: 0.7820, Test: 0.7960\n","Epoch: 065, Loss: 3.4991, Val: 0.7800, Test: 0.7910\n","Epoch: 066, Loss: 3.4811, Val: 0.7620, Test: 0.7890\n","Epoch: 067, Loss: 3.4776, Val: 0.7420, Test: 0.7820\n","Epoch: 068, Loss: 3.4790, Val: 0.7540, Test: 0.7730\n","Epoch: 069, Loss: 3.4946, Val: 0.7640, Test: 0.7700\n","Epoch: 070, Loss: 3.5161, Val: 0.7540, Test: 0.7690\n","Epoch: 071, Loss: 3.4449, Val: 0.7520, Test: 0.7770\n","Epoch: 072, Loss: 3.4766, Val: 0.7500, Test: 0.7810\n","Epoch: 073, Loss: 3.4951, Val: 0.7580, Test: 0.7900\n","Epoch: 074, Loss: 3.5029, Val: 0.7620, Test: 0.7870\n","Epoch: 075, Loss: 3.5254, Val: 0.7560, Test: 0.7830\n","Epoch: 076, Loss: 3.4697, Val: 0.7480, Test: 0.7810\n","Epoch: 077, Loss: 3.4800, Val: 0.7620, Test: 0.7880\n","Epoch: 078, Loss: 3.4844, Val: 0.7680, Test: 0.7990\n","Epoch: 079, Loss: 3.4860, Val: 0.7660, Test: 0.8000\n","Epoch: 080, Loss: 3.4998, Val: 0.7640, Test: 0.8070\n","Epoch: 081, Loss: 3.4572, Val: 0.7720, Test: 0.7990\n","Epoch: 082, Loss: 3.4521, Val: 0.7720, Test: 0.8050\n","Epoch: 083, Loss: 3.5055, Val: 0.7820, Test: 0.8130\n","Epoch: 084, Loss: 3.4849, Val: 0.7680, Test: 0.7960\n","Epoch: 085, Loss: 3.4837, Val: 0.7800, Test: 0.8020\n","Epoch: 086, Loss: 3.5020, Val: 0.7720, Test: 0.8090\n","Epoch: 087, Loss: 3.4716, Val: 0.7840, Test: 0.8020\n","Epoch: 088, Loss: 3.5180, Val: 0.7740, Test: 0.7960\n","Epoch: 089, Loss: 3.4777, Val: 0.7680, Test: 0.7990\n","Epoch: 090, Loss: 3.4511, Val: 0.7700, Test: 0.7930\n","Epoch: 091, Loss: 3.4762, Val: 0.7820, Test: 0.7950\n","Epoch: 092, Loss: 3.5117, Val: 0.8020, Test: 0.8100\n","Epoch: 093, Loss: 3.4944, Val: 0.7900, Test: 0.8100\n","Epoch: 094, Loss: 3.4490, Val: 0.7900, Test: 0.8040\n","Epoch: 095, Loss: 3.4661, Val: 0.7840, Test: 0.8070\n","Epoch: 096, Loss: 3.4568, Val: 0.7900, Test: 0.7970\n","Epoch: 097, Loss: 3.4918, Val: 0.7900, Test: 0.7960\n","Epoch: 098, Loss: 3.4741, Val: 0.7940, Test: 0.7900\n","Epoch: 099, Loss: 3.5049, Val: 0.7820, Test: 0.7920\n","Epoch: 100, Loss: 3.4941, Val: 0.7920, Test: 0.7990\n"]}],"source":["for epoch in range(1, 101):\n","    loss = train()\n","    val_acc, test_acc = test()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n","          f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"]},{"cell_type":"markdown","id":"6caee53d","metadata":{"id":"6caee53d"},"source":["## Performance comparison\n","Recall that in the previous examples, we performed node classification in 3 different ways.\n","1. Bag of word + MLP with `Accuracy:0.6`\n","2. Node2vec + Logistic regression with `Accuracy:0.703`\n","3. Node2vec with Bag of word + Logistic regression with `Accuracy:0.707`\n","4. GraphSAGE: with Bag of word + Logistic regression with `Accuracy:0.803`\n","\n","We make use of node feature and graph structure at the same time and boost the accuracy up to **0.791** with a simple two-layer `GraphSAGE`."]},{"cell_type":"markdown","id":"36240400","metadata":{"id":"36240400"},"source":["## End-to-end semi-supervised learning with Graph Convolution Network(GCN)\n","Previously, we adopt a two stage classification pipeline where we first extract network feature via unsupervised learning then utilize a classifier to predict its label. <br>\n","The two-stage design could be suboptimal since the network features were not extracted for specific task. <br>\n","Therefore, we now employ an end-to-end approach. This ensures that the features learned by the model are directly aligned with the specific task, potentially leading to better performance."]},{"cell_type":"code","execution_count":10,"id":"9b156ae0","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1663386768615,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"9b156ae0"},"outputs":[],"source":["from torch_geometric.nn import GCNConv\n","import torch.nn.functional as F\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels):\n","        super().__init__()\n","        self.conv1 = GCNConv(in_channels, hidden_channels, cached=True,\n","                             normalize=True)\n","        self.conv2 = GCNConv(hidden_channels, out_channels, cached=True,\n","                             normalize=True)\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        x = F.dropout(x, p=0.3, training=self.training)\n","        x = self.conv1(x, edge_index, edge_weight).relu()\n","        x = F.dropout(x, p=0.3, training=self.training)\n","        x = self.conv2(x, edge_index, edge_weight)\n","        return x"]},{"cell_type":"code","execution_count":11,"id":"67338214","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663386768902,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"67338214","outputId":"07232e0d-90e6-43ef-e805-859283b424b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["GCN(\n","  (conv1): GCNConv(1433, 64)\n","  (conv2): GCNConv(64, 7)\n",")\n"]}],"source":["dim = 64\n","model = GCN(dataset.num_features, dim, dataset.num_classes)\n","model, data = model.to(device), data.to(device)\n","optimizer = torch.optim.Adam(model.parameters(),weight_decay=1e-4)\n","print(model)"]},{"cell_type":"code","execution_count":12,"id":"646fb96c","metadata":{"executionInfo":{"elapsed":268,"status":"ok","timestamp":1663386794576,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"646fb96c"},"outputs":[],"source":["def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data.x, data.edge_index, data.edge_weight)\n","    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    return float(loss)\n","\n","@torch.no_grad()\n","def test():\n","    model.eval()\n","    pred = model(data.x, data.edge_index, data.edge_weight).argmax(dim=-1)\n","    mask = data.test_mask\n","    accs = (int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n","    return accs"]},{"cell_type":"code","execution_count":13,"id":"142e2595","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2101,"status":"ok","timestamp":1663386797988,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"142e2595","outputId":"9596e72b-144a-43f7-ba6f-078dcc2338e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss:1.9463 Testing accuracy:0.1710\n","Loss:1.9444 Testing accuracy:0.2670\n","Loss:1.9426 Testing accuracy:0.3370\n","Loss:1.9412 Testing accuracy:0.3930\n","Loss:1.9397 Testing accuracy:0.4380\n","Loss:1.9374 Testing accuracy:0.4690\n","Loss:1.9362 Testing accuracy:0.4950\n","Loss:1.9337 Testing accuracy:0.5230\n","Loss:1.9319 Testing accuracy:0.5370\n","Loss:1.9300 Testing accuracy:0.5460\n","Loss:1.9269 Testing accuracy:0.5540\n","Loss:1.9253 Testing accuracy:0.5570\n","Loss:1.9225 Testing accuracy:0.5650\n","Loss:1.9205 Testing accuracy:0.5640\n","Loss:1.9170 Testing accuracy:0.5700\n","Loss:1.9142 Testing accuracy:0.5690\n","Loss:1.9115 Testing accuracy:0.5740\n","Loss:1.9094 Testing accuracy:0.5690\n","Loss:1.9062 Testing accuracy:0.5630\n","Loss:1.9021 Testing accuracy:0.5590\n","Loss:1.8997 Testing accuracy:0.5580\n","Loss:1.8959 Testing accuracy:0.5530\n","Loss:1.8936 Testing accuracy:0.5490\n","Loss:1.8922 Testing accuracy:0.5450\n","Loss:1.8871 Testing accuracy:0.5470\n","Loss:1.8845 Testing accuracy:0.5460\n","Loss:1.8810 Testing accuracy:0.5460\n","Loss:1.8764 Testing accuracy:0.5560\n","Loss:1.8707 Testing accuracy:0.5620\n","Loss:1.8679 Testing accuracy:0.5670\n","Loss:1.8663 Testing accuracy:0.5690\n","Loss:1.8615 Testing accuracy:0.5700\n","Loss:1.8618 Testing accuracy:0.5790\n","Loss:1.8562 Testing accuracy:0.5840\n","Loss:1.8502 Testing accuracy:0.5920\n","Loss:1.8489 Testing accuracy:0.6000\n","Loss:1.8420 Testing accuracy:0.6070\n","Loss:1.8406 Testing accuracy:0.6110\n","Loss:1.8361 Testing accuracy:0.6200\n","Loss:1.8314 Testing accuracy:0.6290\n","Loss:1.8234 Testing accuracy:0.6380\n","Loss:1.8225 Testing accuracy:0.6440\n","Loss:1.8183 Testing accuracy:0.6460\n","Loss:1.8171 Testing accuracy:0.6500\n","Loss:1.8106 Testing accuracy:0.6570\n","Loss:1.8057 Testing accuracy:0.6630\n","Loss:1.8023 Testing accuracy:0.6750\n","Loss:1.8009 Testing accuracy:0.6790\n","Loss:1.7921 Testing accuracy:0.6820\n","Loss:1.7869 Testing accuracy:0.6860\n","Loss:1.7842 Testing accuracy:0.6910\n","Loss:1.7801 Testing accuracy:0.6940\n","Loss:1.7779 Testing accuracy:0.6970\n","Loss:1.7701 Testing accuracy:0.6980\n","Loss:1.7684 Testing accuracy:0.6990\n","Loss:1.7604 Testing accuracy:0.7000\n","Loss:1.7570 Testing accuracy:0.7040\n","Loss:1.7482 Testing accuracy:0.7050\n","Loss:1.7467 Testing accuracy:0.7060\n","Loss:1.7406 Testing accuracy:0.7070\n","Loss:1.7361 Testing accuracy:0.7090\n","Loss:1.7343 Testing accuracy:0.7110\n","Loss:1.7268 Testing accuracy:0.7110\n","Loss:1.7160 Testing accuracy:0.7120\n","Loss:1.7159 Testing accuracy:0.7150\n","Loss:1.7061 Testing accuracy:0.7180\n","Loss:1.7029 Testing accuracy:0.7180\n","Loss:1.6968 Testing accuracy:0.7180\n","Loss:1.6934 Testing accuracy:0.7180\n","Loss:1.6874 Testing accuracy:0.7190\n","Loss:1.6816 Testing accuracy:0.7200\n","Loss:1.6730 Testing accuracy:0.7200\n","Loss:1.6691 Testing accuracy:0.7270\n","Loss:1.6622 Testing accuracy:0.7280\n","Loss:1.6554 Testing accuracy:0.7290\n","Loss:1.6627 Testing accuracy:0.7320\n","Loss:1.6511 Testing accuracy:0.7320\n","Loss:1.6462 Testing accuracy:0.7340\n","Loss:1.6281 Testing accuracy:0.7340\n","Loss:1.6364 Testing accuracy:0.7340\n","Loss:1.6190 Testing accuracy:0.7360\n","Loss:1.6127 Testing accuracy:0.7360\n","Loss:1.6174 Testing accuracy:0.7360\n","Loss:1.6043 Testing accuracy:0.7390\n","Loss:1.6021 Testing accuracy:0.7410\n","Loss:1.5919 Testing accuracy:0.7430\n","Loss:1.5897 Testing accuracy:0.7440\n","Loss:1.5896 Testing accuracy:0.7430\n","Loss:1.5728 Testing accuracy:0.7440\n","Loss:1.5624 Testing accuracy:0.7440\n","Loss:1.5696 Testing accuracy:0.7460\n","Loss:1.5513 Testing accuracy:0.7480\n","Loss:1.5562 Testing accuracy:0.7470\n","Loss:1.5395 Testing accuracy:0.7490\n","Loss:1.5490 Testing accuracy:0.7510\n","Loss:1.5305 Testing accuracy:0.7530\n","Loss:1.5215 Testing accuracy:0.7530\n","Loss:1.5187 Testing accuracy:0.7520\n","Loss:1.5093 Testing accuracy:0.7540\n","Loss:1.5163 Testing accuracy:0.7550\n","Loss:1.4923 Testing accuracy:0.7570\n","Loss:1.4974 Testing accuracy:0.7620\n","Loss:1.4941 Testing accuracy:0.7620\n","Loss:1.4805 Testing accuracy:0.7630\n","Loss:1.4712 Testing accuracy:0.7670\n","Loss:1.4649 Testing accuracy:0.7700\n","Loss:1.4438 Testing accuracy:0.7700\n","Loss:1.4479 Testing accuracy:0.7710\n","Loss:1.4440 Testing accuracy:0.7710\n","Loss:1.4395 Testing accuracy:0.7730\n","Loss:1.4351 Testing accuracy:0.7750\n","Loss:1.4136 Testing accuracy:0.7750\n","Loss:1.4090 Testing accuracy:0.7740\n","Loss:1.4013 Testing accuracy:0.7740\n","Loss:1.4018 Testing accuracy:0.7750\n","Loss:1.3692 Testing accuracy:0.7750\n","Loss:1.3970 Testing accuracy:0.7750\n","Loss:1.3910 Testing accuracy:0.7750\n","Loss:1.3778 Testing accuracy:0.7740\n","Loss:1.3740 Testing accuracy:0.7750\n","Loss:1.3633 Testing accuracy:0.7760\n","Loss:1.3469 Testing accuracy:0.7780\n","Loss:1.3548 Testing accuracy:0.7800\n","Loss:1.3453 Testing accuracy:0.7800\n","Loss:1.3258 Testing accuracy:0.7820\n","Loss:1.3244 Testing accuracy:0.7830\n","Loss:1.3190 Testing accuracy:0.7830\n","Loss:1.3000 Testing accuracy:0.7830\n","Loss:1.3005 Testing accuracy:0.7850\n","Loss:1.2948 Testing accuracy:0.7870\n","Loss:1.2888 Testing accuracy:0.7870\n","Loss:1.2818 Testing accuracy:0.7880\n","Loss:1.2642 Testing accuracy:0.7880\n","Loss:1.2621 Testing accuracy:0.7890\n","Loss:1.2586 Testing accuracy:0.7900\n","Loss:1.2410 Testing accuracy:0.7900\n","Loss:1.2390 Testing accuracy:0.7900\n","Loss:1.2327 Testing accuracy:0.7900\n","Loss:1.2199 Testing accuracy:0.7900\n","Loss:1.2293 Testing accuracy:0.7910\n","Loss:1.2217 Testing accuracy:0.7910\n","Loss:1.2062 Testing accuracy:0.7910\n","Loss:1.1792 Testing accuracy:0.7930\n","Loss:1.1774 Testing accuracy:0.7930\n","Loss:1.1890 Testing accuracy:0.7920\n","Loss:1.1765 Testing accuracy:0.7920\n","Loss:1.1567 Testing accuracy:0.7920\n","Loss:1.1641 Testing accuracy:0.7920\n","Loss:1.1553 Testing accuracy:0.7910\n","Loss:1.1600 Testing accuracy:0.7910\n","Loss:1.1371 Testing accuracy:0.7900\n","Loss:1.1143 Testing accuracy:0.7900\n","Loss:1.1257 Testing accuracy:0.7910\n","Loss:1.1021 Testing accuracy:0.7920\n","Loss:1.1094 Testing accuracy:0.7930\n","Loss:1.1019 Testing accuracy:0.7920\n","Loss:1.1111 Testing accuracy:0.7930\n","Loss:1.0923 Testing accuracy:0.7940\n","Loss:1.0944 Testing accuracy:0.7940\n","Loss:1.0825 Testing accuracy:0.7950\n","Loss:1.0646 Testing accuracy:0.7960\n","Loss:1.0622 Testing accuracy:0.7960\n","Loss:1.0785 Testing accuracy:0.7960\n","Loss:1.0518 Testing accuracy:0.7960\n","Loss:1.0492 Testing accuracy:0.7960\n","Loss:1.0543 Testing accuracy:0.7970\n","Loss:1.0288 Testing accuracy:0.7970\n","Loss:1.0280 Testing accuracy:0.7970\n","Loss:1.0290 Testing accuracy:0.7970\n","Loss:1.0134 Testing accuracy:0.7980\n","Loss:1.0067 Testing accuracy:0.7970\n","Loss:0.9892 Testing accuracy:0.7970\n","Loss:0.9761 Testing accuracy:0.7970\n","Loss:1.0044 Testing accuracy:0.7970\n","Loss:0.9748 Testing accuracy:0.7970\n","Loss:0.9742 Testing accuracy:0.7980\n","Loss:0.9732 Testing accuracy:0.7980\n","Loss:0.9846 Testing accuracy:0.7990\n","Loss:0.9579 Testing accuracy:0.7990\n","Loss:0.9431 Testing accuracy:0.7990\n","Loss:0.9359 Testing accuracy:0.7990\n","Loss:0.9311 Testing accuracy:0.8000\n","Loss:0.9315 Testing accuracy:0.8020\n","Loss:0.9138 Testing accuracy:0.8020\n","Loss:0.9087 Testing accuracy:0.8030\n","Loss:0.9030 Testing accuracy:0.8030\n","Loss:0.9218 Testing accuracy:0.8030\n","Loss:0.9067 Testing accuracy:0.8030\n","Loss:0.8942 Testing accuracy:0.8030\n","Loss:0.8986 Testing accuracy:0.8020\n","Loss:0.8951 Testing accuracy:0.8010\n","Loss:0.8936 Testing accuracy:0.8000\n","Loss:0.8655 Testing accuracy:0.7980\n","Loss:0.8702 Testing accuracy:0.7980\n","Loss:0.8714 Testing accuracy:0.7990\n","Loss:0.8604 Testing accuracy:0.7990\n","Loss:0.8500 Testing accuracy:0.8010\n","Loss:0.8331 Testing accuracy:0.8030\n","Loss:0.8437 Testing accuracy:0.8000\n","Loss:0.8405 Testing accuracy:0.8000\n"]}],"source":["for epoch in range(200):\n","    loss = train()\n","    test_acc = test()\n","    print(f\"Loss:{loss:.4f} Testing accuracy:{test_acc:.4f}\")"]},{"cell_type":"markdown","id":"ec8ff377","metadata":{"id":"ec8ff377"},"source":["## Performance comparison\n","Recall that in the previous examples, we performed node classification in 3 different ways.\n","1. Bag of word + MLP with `Accuracy:0.6`\n","2. Node2vec + Logistic regression with `Accuracy:0.703`\n","3. Node2vec with Bag of word + Logistic regression with `Accuracy:0.707`\n","4. GraphSAGE: with Bag of word + Logistic regression with `Accuracy:0.803`\n","4. GCN: end-to-end learning with `Accuracy:0.818`\n","\n","From this example, we clearly figure out that using GCN with end2end training leads to the best performance since the feature extracted could be learned to optimize the node classification task."]},{"cell_type":"markdown","id":"d347b13c","metadata":{"id":"d347b13c"},"source":["## Applying different GNN backbone layer\n","The full list of implemented GNN could be found in [here.](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers)"]},{"cell_type":"code","execution_count":14,"id":"765f7d34","metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1663387016492,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"765f7d34"},"outputs":[],"source":["from torch_geometric.nn import GCNConv,GATConv, GraphSAGE\n","import torch.nn.functional as F\n","\n","class GNN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, gnn_type):\n","        super().__init__()\n","        if gnn_type == \"GCN\":\n","            self.GNN = GCNConv\n","        elif gnn_type == \"SAGE\":\n","            self.GNN = GraphSAGE\n","        elif gnn_type == \"GAT\":\n","            self.GNN = GATConv\n","        \n","        self.conv1 = self.GNN(in_channels, hidden_channels)\n","        self.conv2 = self.GNN(hidden_channels, out_channels)\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv1(x, edge_index, edge_weight).relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index, edge_weight)\n","        return x"]},{"cell_type":"code","execution_count":15,"id":"f37bd755","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1663387016912,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"f37bd755","outputId":"7c33b9d5-4dd0-4305-ca6e-c1bee2a074d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["GNN(\n","  (conv1): GATConv(1433, 32, heads=1)\n","  (conv2): GATConv(32, 7, heads=1)\n",")\n"]}],"source":["dim = 32\n","gnn_type = \"GAT\"\n","model = GNN(dataset.num_features, dim, dataset.num_classes,gnn_type=gnn_type)\n","model, data = model.to(device), data.to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","print(model)"]},{"cell_type":"markdown","id":"afe00d31","metadata":{"id":"afe00d31"},"source":["# HW: Link prediction with GNN\n","1. Try different GNN layer\n","2. Try to optimize the performance by stacking multiple layers\n","3. Report the best accuracy on testing set and the best model configuration(e.g., how many layers?)"]},{"cell_type":"code","execution_count":16,"id":"cfb91924","metadata":{"executionInfo":{"elapsed":281,"status":"ok","timestamp":1663387050805,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"cfb91924"},"outputs":[],"source":["# Let's practice how to use GNN for link prediction\n","# First we need to load the Cora dataset\n","\n","from sklearn.metrics import roc_auc_score\n","import torch_geometric.transforms as T\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.utils import negative_sampling\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","transform = T.Compose([\n","    T.NormalizeFeatures(),\n","    T.ToDevice(device),\n","    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n","                      add_negative_train_samples=True),\n","])\n","dataset = Planetoid(path, name='Cora', transform=transform)\n","train_data, val_data, test_data = dataset[0]"]},{"cell_type":"code","execution_count":17,"id":"43bed8c6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1663387060011,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"43bed8c6","outputId":"24659c22-d453-4534-db45-9d742e94c9b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------Training data------\n","Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[8976], edge_label_index=[2, 8976])\n","Training edges:\n","tensor([[1114, 1574, 1309,  ..., 1106, 2504, 2288],\n","        [1717, 1986, 2103,  ..., 1287,  539,  837]], device='cuda:0')\n","Labels\n","tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0')\n","\n","--------Testing data------\n","Data(x=[2708, 1433], edge_index=[2, 9502], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054])\n","Testing edges:\n","tensor([[ 718,  778, 1624,  ..., 1014, 2392, 1564],\n","        [2261, 1370, 1779,  ..., 1513, 1025,  763]], device='cuda:0')\n","Labels\n","tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0')\n"]}],"source":["print(\"--------Training data------\")\n","print(train_data)\n","print(\"Training edges:\")\n","print(train_data.edge_label_index)\n","print(\"Labels\")\n","print(train_data.edge_label)\n","\n","print()\n","print(\"--------Testing data------\")\n","print(test_data)\n","print(\"Testing edges:\")\n","print(test_data.edge_label_index)\n","print(\"Labels\")\n","print(test_data.edge_label)"]},{"cell_type":"code","execution_count":18,"id":"8ccc3487","metadata":{"executionInfo":{"elapsed":1021,"status":"ok","timestamp":1663389253599,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"8ccc3487"},"outputs":[],"source":["class MyGNN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels):\n","        super().__init__()\n","        ############################################################################\n","        # TODO: Your code here! \n","        # create you GNN layer here. \n","        # try to use different GNN backbone layer or stacking multiple layer to boost performance\n","        self.conv1 = GCNConv(in_channels, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, out_channels)\n","        \n","        ############################################################################\n","\n","    def forward(self, x, edge_index):\n","        ############################################################################\n","        # TODO: Your code here! \n","        # Apply the forward pass according to your GNN layers\n","        # you shoud return the embedding of each node (x has shape [num_nodes, dim])    \n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = self.conv2(x, edge_index)  \n","        ############################################################################\n","        return x\n","    \n","    def get_prediction(self, node_embedding, edges):\n","        # In this function, we have the node embedding and edges as input\n","        # Input shapes:\n","        #      node_embedding: (|V|, out_channels)\n","        #      edges: (2, number of edges)\n","        # To generate such output, we use the inner product of embeddings of two nodes\n","        # The output is to generate a scalar for each pair of edge\n","        embedding_first_node = node_embedding[edges[0]]\n","        embedding_second_node = node_embedding[edges[1]]\n","        ############################################################################\n","        # TODO: Your code here! \n","        # implement the element-wise product as edge feature for link prediction\n","        inner_product = torch.sum(embedding_first_node * embedding_second_node, dim=-1)\n","        \n","        ############################################################################\n","        return inner_product"]},{"cell_type":"code","execution_count":19,"id":"8236b1d0","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663389254662,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"8236b1d0"},"outputs":[],"source":["############################################################################\n","# TODO: Your code here! \n","# initiate your GNN model and select the criterion for link prediction\n","\n","model = MyGNN(dataset.num_features, 128, 64).to(device)\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","############################################################################"]},{"cell_type":"code","execution_count":20,"id":"52305879","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663389254663,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"52305879"},"outputs":[],"source":["# Implement the train function\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    embedding = model(train_data.x, train_data.edge_index)\n","\n","    # We perform a new round of negative sampling for every training epoch:\n","    neg_edge_index = negative_sampling(\n","        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n","        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n","\n","    edge_label_index = torch.cat(\n","        [train_data.edge_label_index, neg_edge_index],\n","        dim=-1,\n","    )\n","    \n","    # Please assign the target for negative edges\n","    edge_label = torch.cat([\n","        train_data.edge_label,\n","        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n","    ], dim=0)\n","    \n","    # make prediction\n","    prediction = model.get_prediction(embedding, edge_label_index).view(-1)\n","    \n","    # optimization\n","    loss = criterion(prediction, edge_label)\n","    loss.backward()\n","    optimizer.step()\n","    return loss"]},{"cell_type":"code","execution_count":21,"id":"ad8d6158","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663389254663,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"ad8d6158"},"outputs":[],"source":["# Implement the test function\n","@torch.no_grad()\n","def test(data):\n","    model.eval()\n","    embedding = model(data.x, data.edge_index)\n","    \n","    # use the sigmoid function to normalize our prediction into [0,1]\n","    out = model.get_prediction(embedding, data.edge_label_index).view(-1).sigmoid()\n","    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())"]},{"cell_type":"code","execution_count":22,"id":"f60a1f15","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2360,"status":"ok","timestamp":1663389257017,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"},"user_tz":-480},"id":"f60a1f15","outputId":"c3f96ee5-852a-4357-950d-add431c7cd23","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 001, Loss: 0.6932, Val: 0.6865, Test: 0.7229\n","Epoch: 002, Loss: 0.6989, Val: 0.7523, Test: 0.7769\n","Epoch: 003, Loss: 0.6936, Val: 0.7302, Test: 0.7392\n","Epoch: 004, Loss: 0.6934, Val: 0.5911, Test: 0.6068\n","Epoch: 005, Loss: 0.6939, Val: 0.5107, Test: 0.5543\n","Epoch: 006, Loss: 0.6942, Val: 0.6673, Test: 0.6352\n","Epoch: 007, Loss: 0.6941, Val: 0.6293, Test: 0.6397\n","Epoch: 008, Loss: 0.6937, Val: 0.6867, Test: 0.6698\n","Epoch: 009, Loss: 0.6934, Val: 0.7193, Test: 0.6710\n","Epoch: 010, Loss: 0.6933, Val: 0.6632, Test: 0.6265\n","Epoch: 011, Loss: 0.6933, Val: 0.6099, Test: 0.6025\n","Epoch: 012, Loss: 0.6933, Val: 0.6196, Test: 0.6129\n","Epoch: 013, Loss: 0.6933, Val: 0.6481, Test: 0.6465\n","Epoch: 014, Loss: 0.6932, Val: 0.7004, Test: 0.6939\n","Epoch: 015, Loss: 0.6930, Val: 0.7132, Test: 0.7019\n","Epoch: 016, Loss: 0.6927, Val: 0.7336, Test: 0.7233\n","Epoch: 017, Loss: 0.6923, Val: 0.7363, Test: 0.7390\n","Epoch: 018, Loss: 0.6917, Val: 0.7383, Test: 0.7420\n","Epoch: 019, Loss: 0.6910, Val: 0.7537, Test: 0.7467\n","Epoch: 020, Loss: 0.6902, Val: 0.7599, Test: 0.7513\n","Epoch: 021, Loss: 0.6891, Val: 0.7587, Test: 0.7511\n","Epoch: 022, Loss: 0.6875, Val: 0.7526, Test: 0.7525\n","Epoch: 023, Loss: 0.6855, Val: 0.7617, Test: 0.7545\n","Epoch: 024, Loss: 0.6832, Val: 0.7693, Test: 0.7570\n","Epoch: 025, Loss: 0.6798, Val: 0.7667, Test: 0.7580\n","Epoch: 026, Loss: 0.6755, Val: 0.7660, Test: 0.7593\n","Epoch: 027, Loss: 0.6715, Val: 0.7715, Test: 0.7625\n","Epoch: 028, Loss: 0.6654, Val: 0.7696, Test: 0.7622\n","Epoch: 029, Loss: 0.6617, Val: 0.7794, Test: 0.7699\n","Epoch: 030, Loss: 0.6602, Val: 0.7775, Test: 0.7697\n","Epoch: 031, Loss: 0.6576, Val: 0.7794, Test: 0.7731\n","Epoch: 032, Loss: 0.6574, Val: 0.7867, Test: 0.7790\n","Epoch: 033, Loss: 0.6576, Val: 0.7874, Test: 0.7805\n","Epoch: 034, Loss: 0.6562, Val: 0.7940, Test: 0.7856\n","Epoch: 035, Loss: 0.6503, Val: 0.7998, Test: 0.7909\n","Epoch: 036, Loss: 0.6466, Val: 0.8073, Test: 0.7979\n","Epoch: 037, Loss: 0.6439, Val: 0.8161, Test: 0.8066\n","Epoch: 038, Loss: 0.6435, Val: 0.8183, Test: 0.8122\n","Epoch: 039, Loss: 0.6397, Val: 0.8226, Test: 0.8197\n","Epoch: 040, Loss: 0.6381, Val: 0.8261, Test: 0.8262\n","Epoch: 041, Loss: 0.6328, Val: 0.8255, Test: 0.8291\n","Epoch: 042, Loss: 0.6318, Val: 0.8264, Test: 0.8324\n","Epoch: 043, Loss: 0.6293, Val: 0.8274, Test: 0.8355\n","Epoch: 044, Loss: 0.6302, Val: 0.8258, Test: 0.8356\n","Epoch: 045, Loss: 0.6293, Val: 0.8241, Test: 0.8354\n","Epoch: 046, Loss: 0.6255, Val: 0.8259, Test: 0.8382\n","Epoch: 047, Loss: 0.6253, Val: 0.8245, Test: 0.8391\n","Epoch: 048, Loss: 0.6293, Val: 0.8325, Test: 0.8436\n","Epoch: 049, Loss: 0.6257, Val: 0.8259, Test: 0.8427\n","Epoch: 050, Loss: 0.6203, Val: 0.8261, Test: 0.8435\n","Epoch: 051, Loss: 0.6192, Val: 0.8323, Test: 0.8471\n","Epoch: 052, Loss: 0.6194, Val: 0.8311, Test: 0.8476\n","Epoch: 053, Loss: 0.6206, Val: 0.8266, Test: 0.8462\n","Epoch: 054, Loss: 0.6190, Val: 0.8264, Test: 0.8467\n","Epoch: 055, Loss: 0.6180, Val: 0.8288, Test: 0.8475\n","Epoch: 056, Loss: 0.6156, Val: 0.8287, Test: 0.8477\n","Epoch: 057, Loss: 0.6178, Val: 0.8275, Test: 0.8481\n","Epoch: 058, Loss: 0.6157, Val: 0.8268, Test: 0.8490\n","Epoch: 059, Loss: 0.6173, Val: 0.8267, Test: 0.8498\n","Epoch: 060, Loss: 0.6154, Val: 0.8269, Test: 0.8494\n","Epoch: 061, Loss: 0.6118, Val: 0.8272, Test: 0.8504\n","Epoch: 062, Loss: 0.6148, Val: 0.8245, Test: 0.8505\n","Epoch: 063, Loss: 0.6133, Val: 0.8248, Test: 0.8506\n","Epoch: 064, Loss: 0.6131, Val: 0.8253, Test: 0.8493\n","Epoch: 065, Loss: 0.6095, Val: 0.8263, Test: 0.8508\n","Epoch: 066, Loss: 0.6137, Val: 0.8233, Test: 0.8510\n","Epoch: 067, Loss: 0.6093, Val: 0.8237, Test: 0.8511\n","Epoch: 068, Loss: 0.6079, Val: 0.8245, Test: 0.8498\n","Epoch: 069, Loss: 0.6059, Val: 0.8255, Test: 0.8501\n","Epoch: 070, Loss: 0.6060, Val: 0.8252, Test: 0.8518\n","Epoch: 071, Loss: 0.6043, Val: 0.8235, Test: 0.8520\n","Epoch: 072, Loss: 0.6082, Val: 0.8259, Test: 0.8512\n","Epoch: 073, Loss: 0.6067, Val: 0.8263, Test: 0.8500\n","Epoch: 074, Loss: 0.6054, Val: 0.8253, Test: 0.8525\n","Epoch: 075, Loss: 0.6034, Val: 0.8233, Test: 0.8530\n","Epoch: 076, Loss: 0.6015, Val: 0.8260, Test: 0.8536\n","Epoch: 077, Loss: 0.6051, Val: 0.8269, Test: 0.8519\n","Epoch: 078, Loss: 0.6022, Val: 0.8248, Test: 0.8539\n","Epoch: 079, Loss: 0.6020, Val: 0.8242, Test: 0.8563\n","Epoch: 080, Loss: 0.6014, Val: 0.8248, Test: 0.8580\n","Epoch: 081, Loss: 0.5999, Val: 0.8259, Test: 0.8574\n","Epoch: 082, Loss: 0.5993, Val: 0.8273, Test: 0.8579\n","Epoch: 083, Loss: 0.5975, Val: 0.8269, Test: 0.8604\n","Epoch: 084, Loss: 0.5980, Val: 0.8279, Test: 0.8625\n","Epoch: 085, Loss: 0.5974, Val: 0.8299, Test: 0.8623\n","Epoch: 086, Loss: 0.5977, Val: 0.8302, Test: 0.8605\n","Epoch: 087, Loss: 0.5954, Val: 0.8307, Test: 0.8624\n","Epoch: 088, Loss: 0.5963, Val: 0.8302, Test: 0.8646\n","Epoch: 089, Loss: 0.5954, Val: 0.8315, Test: 0.8634\n","Epoch: 090, Loss: 0.5950, Val: 0.8331, Test: 0.8610\n","Epoch: 091, Loss: 0.5913, Val: 0.8334, Test: 0.8618\n","Epoch: 092, Loss: 0.5921, Val: 0.8329, Test: 0.8633\n","Epoch: 093, Loss: 0.5948, Val: 0.8343, Test: 0.8626\n","Epoch: 094, Loss: 0.5905, Val: 0.8348, Test: 0.8616\n","Epoch: 095, Loss: 0.5912, Val: 0.8343, Test: 0.8620\n","Epoch: 096, Loss: 0.5904, Val: 0.8334, Test: 0.8630\n","Epoch: 097, Loss: 0.5904, Val: 0.8355, Test: 0.8637\n","Epoch: 098, Loss: 0.5864, Val: 0.8365, Test: 0.8638\n","Epoch: 099, Loss: 0.5841, Val: 0.8356, Test: 0.8645\n","Epoch: 100, Loss: 0.5875, Val: 0.8361, Test: 0.8660\n","Final Test: 0.8660\n"]}],"source":["best_val_auc = final_test_auc = 0\n","for epoch in range(1, 101):\n","    loss = train()\n","    val_auc = test(val_data)\n","    test_auc = test(test_data)\n","    if val_auc > best_val_auc:\n","        best_val = val_auc\n","        final_test_auc = test_auc\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n","          f'Test: {test_auc:.4f}')\n","\n","print(f'Final Test: {final_test_auc:.4f}')\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":5}
